{"custom_metrics": {}, "episode_media": {}, "num_recreated_workers": 0, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 20.495146904116677, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.706481564333362, "policy_loss": -0.012549552888238943, "vf_loss": 1.7164510254714838, "vf_explained_var": -0.06170719112119367, "kl": 0.012900411067477966, "entropy": 3.3053327921898132, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 128.0}}, "num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_agent_steps_sampled": 4000, "num_agent_steps_trained": 4000}, "sampler_results": {"episode_reward_max": 139.9889570825846, "episode_reward_min": 0.0, "episode_reward_mean": 46.156572597168626, "episode_len_mean": 11.0, "episode_media": {}, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [2.455653014163037, 28.184834209212944, 70.06589466719613, 16.115222905444927, 128.87359872707214, 76.57782796348005, 139.9889570825846, 49.03701177897837, 55.25219281866832, 58.935672339912884, 102.49641580502396, 7.462153966366274, 0.0, 0.0, 57.88689996420695, 18.04904965409832, 0.0, 87.44447933568603, 117.10478635486467, 60.331514818071334, 0.0, 19.4088675106911, 51.007553437096675, 49.40927342809415, 0.0, 8.618787831153046, 28.427253205204853, 93.06253245024168, 99.7279300329522, 0.0, 45.76723305146359, 23.689377671004046, 59.6972317309302, 17.40444073788052, 0.0, 89.15396700632738], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.211831524597471, "mean_inference_ms": 8.195199776644728, "mean_action_processing_ms": 0.8236810342589422, "mean_env_wait_ms": 1.274671127547079, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0}, "episode_reward_max": 139.9889570825846, "episode_reward_min": 0.0, "episode_reward_mean": 46.156572597168626, "episode_len_mean": 11.0, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "hist_stats": {"episode_reward": [2.455653014163037, 28.184834209212944, 70.06589466719613, 16.115222905444927, 128.87359872707214, 76.57782796348005, 139.9889570825846, 49.03701177897837, 55.25219281866832, 58.935672339912884, 102.49641580502396, 7.462153966366274, 0.0, 0.0, 57.88689996420695, 18.04904965409832, 0.0, 87.44447933568603, 117.10478635486467, 60.331514818071334, 0.0, 19.4088675106911, 51.007553437096675, 49.40927342809415, 0.0, 8.618787831153046, 28.427253205204853, 93.06253245024168, 99.7279300329522, 0.0, 45.76723305146359, 23.689377671004046, 59.6972317309302, 17.40444073788052, 0.0, 89.15396700632738], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.211831524597471, "mean_inference_ms": 8.195199776644728, "mean_action_processing_ms": 0.8236810342589422, "mean_env_wait_ms": 1.274671127547079, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "num_agent_steps_sampled": 4000, "num_agent_steps_trained": 4000, "num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "timesteps_total": 4000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 4000, "timers": {"training_iteration_time_ms": 63775.344, "load_time_ms": 0.385, "load_throughput": 10381940.594, "learn_time_ms": 58897.831, "learn_throughput": 67.914, "synch_weights_time_ms": 2.347}, "counters": {"num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_agent_steps_sampled": 4000, "num_agent_steps_trained": 4000}, "done": false, "episodes_total": 36, "training_iteration": 1, "trial_id": "22332_00000", "experiment_id": "2959d3d26cb84e63b4d3f3fc08306aff", "date": "2022-12-14_12-32-48", "timestamp": 1671039168, "time_this_iter_s": 63.783543825149536, "time_total_s": 63.783543825149536, "pid": 969334, "hostname": "rhino.zoo.cs.yale.edu", "node_ip": "128.36.108.32", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f3d2c6b16f0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f3d2c4f4340>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 63.783543825149536, "timesteps_since_restore": 0, "iterations_since_restore": 1, "warmup_time": 10.023817300796509, "perf": {"cpu_util_percent": 18.571428571428573, "ram_util_percent": 29.074725274725267}}
{"custom_metrics": {}, "episode_media": {}, "num_recreated_workers": 0, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 21.82241697593402, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 2.0347902954906547, "policy_loss": -0.018640833028582156, "vf_loss": 2.0507010737474087, "vf_explained_var": 0.007460042033144223, "kl": 0.013650234129800425, "entropy": 3.120315502535912, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 128.0}}, "num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_agent_steps_sampled": 8000, "num_agent_steps_trained": 8000}, "sampler_results": {"episode_reward_max": 176.41143275693588, "episode_reward_min": 0.0, "episode_reward_mean": 57.884077448736875, "episode_len_mean": 11.0, "episode_media": {}, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [2.455653014163037, 28.184834209212944, 70.06589466719613, 16.115222905444927, 128.87359872707214, 76.57782796348005, 139.9889570825846, 49.03701177897837, 55.25219281866832, 58.935672339912884, 102.49641580502396, 7.462153966366274, 0.0, 0.0, 57.88689996420695, 18.04904965409832, 0.0, 87.44447933568603, 117.10478635486467, 60.331514818071334, 0.0, 19.4088675106911, 51.007553437096675, 49.40927342809415, 0.0, 8.618787831153046, 28.427253205204853, 93.06253245024168, 99.7279300329522, 0.0, 45.76723305146359, 23.689377671004046, 59.6972317309302, 17.40444073788052, 0.0, 89.15396700632738, 1.2892178324355943, 0.0, 98.62516418132296, 44.062242468067524, 9.951688852992223, 50.27949546498817, 56.15270859690621, 80.5993779291942, 95.66481384881564, 42.54418847037461, 75.44181693841557, 52.79653980450529, 69.12752880592556, 22.21296681938081, 63.632569164439836, 10.436525310192906, 59.15576024130725, 102.45090089607515, 148.26005073009333, 52.1166308762089, 67.15938569729647, 77.26991539594356, 125.04009754684392, 126.35491830873437, 37.421397110431954, 176.41143275693588, 0.0, 39.9598344898914, 129.84529530099746, 105.82833284485373, 55.952053927704796, 114.65299730251795, 62.05926297392826, 69.84126984126983, 134.68814801592808, 48.732434066065466], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.178814546184661, "mean_inference_ms": 8.213179468453383, "mean_action_processing_ms": 0.8232768132928254, "mean_env_wait_ms": 1.270678614675566, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0}, "episode_reward_max": 176.41143275693588, "episode_reward_min": 0.0, "episode_reward_mean": 57.884077448736875, "episode_len_mean": 11.0, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "hist_stats": {"episode_reward": [2.455653014163037, 28.184834209212944, 70.06589466719613, 16.115222905444927, 128.87359872707214, 76.57782796348005, 139.9889570825846, 49.03701177897837, 55.25219281866832, 58.935672339912884, 102.49641580502396, 7.462153966366274, 0.0, 0.0, 57.88689996420695, 18.04904965409832, 0.0, 87.44447933568603, 117.10478635486467, 60.331514818071334, 0.0, 19.4088675106911, 51.007553437096675, 49.40927342809415, 0.0, 8.618787831153046, 28.427253205204853, 93.06253245024168, 99.7279300329522, 0.0, 45.76723305146359, 23.689377671004046, 59.6972317309302, 17.40444073788052, 0.0, 89.15396700632738, 1.2892178324355943, 0.0, 98.62516418132296, 44.062242468067524, 9.951688852992223, 50.27949546498817, 56.15270859690621, 80.5993779291942, 95.66481384881564, 42.54418847037461, 75.44181693841557, 52.79653980450529, 69.12752880592556, 22.21296681938081, 63.632569164439836, 10.436525310192906, 59.15576024130725, 102.45090089607515, 148.26005073009333, 52.1166308762089, 67.15938569729647, 77.26991539594356, 125.04009754684392, 126.35491830873437, 37.421397110431954, 176.41143275693588, 0.0, 39.9598344898914, 129.84529530099746, 105.82833284485373, 55.952053927704796, 114.65299730251795, 62.05926297392826, 69.84126984126983, 134.68814801592808, 48.732434066065466], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.178814546184661, "mean_inference_ms": 8.213179468453383, "mean_action_processing_ms": 0.8232768132928254, "mean_env_wait_ms": 1.270678614675566, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "num_agent_steps_sampled": 8000, "num_agent_steps_trained": 8000, "num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "timesteps_total": 8000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 8000, "timers": {"training_iteration_time_ms": 64665.069, "load_time_ms": 0.707, "load_throughput": 5660329.285, "learn_time_ms": 59849.057, "learn_throughput": 66.835, "synch_weights_time_ms": 2.398}, "counters": {"num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_agent_steps_sampled": 8000, "num_agent_steps_trained": 8000}, "done": false, "episodes_total": 72, "training_iteration": 2, "trial_id": "22332_00000", "experiment_id": "2959d3d26cb84e63b4d3f3fc08306aff", "date": "2022-12-14_12-33-55", "timestamp": 1671039235, "time_this_iter_s": 65.5619707107544, "time_total_s": 129.34551453590393, "pid": 969334, "hostname": "rhino.zoo.cs.yale.edu", "node_ip": "128.36.108.32", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f3d2c4d9660>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f3d2c57ebf0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 129.34551453590393, "timesteps_since_restore": 0, "iterations_since_restore": 2, "warmup_time": 10.023817300796509, "perf": {"cpu_util_percent": 23.958947368421057, "ram_util_percent": 29.222105263157907}}
{"custom_metrics": {}, "episode_media": {}, "num_recreated_workers": 0, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 19.46167460180098, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 2.670462478104458, "policy_loss": -0.01574995008182101, "vf_loss": 2.6835681840937626, "vf_explained_var": 0.0005538009828136814, "kl": 0.013221280822294195, "entropy": 3.088452604765533, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 128.0}}, "num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_agent_steps_sampled": 12000, "num_agent_steps_trained": 12000}, "sampler_results": {"episode_reward_max": 243.07761336204067, "episode_reward_min": 0.0, "episode_reward_mean": 73.3367226776272, "episode_len_mean": 11.0, "episode_media": {}, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [55.25219281866832, 58.935672339912884, 102.49641580502396, 7.462153966366274, 0.0, 0.0, 57.88689996420695, 18.04904965409832, 0.0, 87.44447933568603, 117.10478635486467, 60.331514818071334, 0.0, 19.4088675106911, 51.007553437096675, 49.40927342809415, 0.0, 8.618787831153046, 28.427253205204853, 93.06253245024168, 99.7279300329522, 0.0, 45.76723305146359, 23.689377671004046, 59.6972317309302, 17.40444073788052, 0.0, 89.15396700632738, 1.2892178324355943, 0.0, 98.62516418132296, 44.062242468067524, 9.951688852992223, 50.27949546498817, 56.15270859690621, 80.5993779291942, 95.66481384881564, 42.54418847037461, 75.44181693841557, 52.79653980450529, 69.12752880592556, 22.21296681938081, 63.632569164439836, 10.436525310192906, 59.15576024130725, 102.45090089607515, 148.26005073009333, 52.1166308762089, 67.15938569729647, 77.26991539594356, 125.04009754684392, 126.35491830873437, 37.421397110431954, 176.41143275693588, 0.0, 39.9598344898914, 129.84529530099746, 105.82833284485373, 55.952053927704796, 114.65299730251795, 62.05926297392826, 69.84126984126983, 134.68814801592808, 48.732434066065466, 71.95063331497697, 80.72562358276643, 142.5938921889433, 58.88502449649577, 73.77003078475875, 51.48886236792729, 243.07761336204067, 133.16515932594885, 0.0, 14.619730219819639, 206.07637459681266, 23.505784994053762, 181.4535557583118, 133.59700681990958, 76.51754438703763, 40.61036172172122, 153.42873176856548, 18.618865202088223, 52.770487311650704, 71.2637550515785, 202.8769213851414, 55.43314375014946, 81.69314256515548, 136.813232848747, 81.77342992711871, 93.83705251167882, 113.17744068240981, 24.972827573073367, 109.21141526013878, 25.91512795594428, 76.34471742719495, 154.5857950133946, 174.31767324332534, 195.4162933969301, 139.61774167354832, 183.21269933243843], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.182424298723436, "mean_inference_ms": 8.24505039407838, "mean_action_processing_ms": 0.8247401082165644, "mean_env_wait_ms": 1.273040842687667, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0}, "episode_reward_max": 243.07761336204067, "episode_reward_min": 0.0, "episode_reward_mean": 73.3367226776272, "episode_len_mean": 11.0, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "hist_stats": {"episode_reward": [55.25219281866832, 58.935672339912884, 102.49641580502396, 7.462153966366274, 0.0, 0.0, 57.88689996420695, 18.04904965409832, 0.0, 87.44447933568603, 117.10478635486467, 60.331514818071334, 0.0, 19.4088675106911, 51.007553437096675, 49.40927342809415, 0.0, 8.618787831153046, 28.427253205204853, 93.06253245024168, 99.7279300329522, 0.0, 45.76723305146359, 23.689377671004046, 59.6972317309302, 17.40444073788052, 0.0, 89.15396700632738, 1.2892178324355943, 0.0, 98.62516418132296, 44.062242468067524, 9.951688852992223, 50.27949546498817, 56.15270859690621, 80.5993779291942, 95.66481384881564, 42.54418847037461, 75.44181693841557, 52.79653980450529, 69.12752880592556, 22.21296681938081, 63.632569164439836, 10.436525310192906, 59.15576024130725, 102.45090089607515, 148.26005073009333, 52.1166308762089, 67.15938569729647, 77.26991539594356, 125.04009754684392, 126.35491830873437, 37.421397110431954, 176.41143275693588, 0.0, 39.9598344898914, 129.84529530099746, 105.82833284485373, 55.952053927704796, 114.65299730251795, 62.05926297392826, 69.84126984126983, 134.68814801592808, 48.732434066065466, 71.95063331497697, 80.72562358276643, 142.5938921889433, 58.88502449649577, 73.77003078475875, 51.48886236792729, 243.07761336204067, 133.16515932594885, 0.0, 14.619730219819639, 206.07637459681266, 23.505784994053762, 181.4535557583118, 133.59700681990958, 76.51754438703763, 40.61036172172122, 153.42873176856548, 18.618865202088223, 52.770487311650704, 71.2637550515785, 202.8769213851414, 55.43314375014946, 81.69314256515548, 136.813232848747, 81.77342992711871, 93.83705251167882, 113.17744068240981, 24.972827573073367, 109.21141526013878, 25.91512795594428, 76.34471742719495, 154.5857950133946, 174.31767324332534, 195.4162933969301, 139.61774167354832, 183.21269933243843], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.182424298723436, "mean_inference_ms": 8.24505039407838, "mean_action_processing_ms": 0.8247401082165644, "mean_env_wait_ms": 1.273040842687667, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "num_agent_steps_sampled": 12000, "num_agent_steps_trained": 12000, "num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "timesteps_total": 12000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 12000, "timers": {"training_iteration_time_ms": 67438.334, "load_time_ms": 0.818, "load_throughput": 4891316.618, "learn_time_ms": 62599.47, "learn_throughput": 63.898, "synch_weights_time_ms": 2.32}, "counters": {"num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_agent_steps_sampled": 12000, "num_agent_steps_trained": 12000}, "done": true, "episodes_total": 108, "training_iteration": 3, "trial_id": "22332_00000", "experiment_id": "2959d3d26cb84e63b4d3f3fc08306aff", "date": "2022-12-14_12-35-08", "timestamp": 1671039308, "time_this_iter_s": 72.99219155311584, "time_total_s": 202.33770608901978, "pid": 969334, "hostname": "rhino.zoo.cs.yale.edu", "node_ip": "128.36.108.32", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f3d2c51b400>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f3d2c3eeb60>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 202.33770608901978, "timesteps_since_restore": 0, "iterations_since_restore": 3, "warmup_time": 10.023817300796509, "perf": {"cpu_util_percent": 19.360576923076923, "ram_util_percent": 25.701923076923087}}
