{"custom_metrics": {}, "episode_media": {}, "num_recreated_workers": 0, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 21.346428512886007, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.2349151707785104, "policy_loss": -0.01565498489365783, "vf_loss": 1.2477244944992725, "vf_explained_var": -0.15857305430596874, "kl": 0.014228286331009228, "entropy": 3.2895553376085016, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 128.0}}, "num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_agent_steps_sampled": 4000, "num_agent_steps_trained": 4000}, "sampler_results": {"episode_reward_max": 149.9892947239621, "episode_reward_min": 0.0, "episode_reward_mean": 47.9964181712425, "episode_len_mean": 11.0, "episode_media": {}, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [64.67200104184104, 50.103033774173554, 134.9110480429042, 17.767033253253032, 99.26977309754075, 52.62870785680384, 34.80888147576104, 21.5959399632869, 0.0, 1.4213626602602427, 84.25535810457414, 70.75241283210184, 0.0, 25.37132348564533, 129.45438713033838, 39.916526822591855, 61.33246949573479, 0.0, 125.87347864315792, 0.0, 0.0, 0.0, 0.0, 10.969366330558424, 60.77741210053516, 24.187452758881328, 20.259137366845053, 107.15011709830333, 11.50626915448768, 0.0, 97.75494214442894, 149.9892947239621, 68.02721088435374, 98.717020951884, 64.39909297052154, 0.0], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.218554098214675, "mean_inference_ms": 8.224244734541099, "mean_action_processing_ms": 0.8226816927022601, "mean_env_wait_ms": 1.3041241252007174, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0}, "episode_reward_max": 149.9892947239621, "episode_reward_min": 0.0, "episode_reward_mean": 47.9964181712425, "episode_len_mean": 11.0, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "hist_stats": {"episode_reward": [64.67200104184104, 50.103033774173554, 134.9110480429042, 17.767033253253032, 99.26977309754075, 52.62870785680384, 34.80888147576104, 21.5959399632869, 0.0, 1.4213626602602427, 84.25535810457414, 70.75241283210184, 0.0, 25.37132348564533, 129.45438713033838, 39.916526822591855, 61.33246949573479, 0.0, 125.87347864315792, 0.0, 0.0, 0.0, 0.0, 10.969366330558424, 60.77741210053516, 24.187452758881328, 20.259137366845053, 107.15011709830333, 11.50626915448768, 0.0, 97.75494214442894, 149.9892947239621, 68.02721088435374, 98.717020951884, 64.39909297052154, 0.0], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.218554098214675, "mean_inference_ms": 8.224244734541099, "mean_action_processing_ms": 0.8226816927022601, "mean_env_wait_ms": 1.3041241252007174, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "num_agent_steps_sampled": 4000, "num_agent_steps_trained": 4000, "num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "timesteps_total": 4000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 4000, "timers": {"training_iteration_time_ms": 65101.207, "load_time_ms": 0.369, "load_throughput": 10845000.646, "learn_time_ms": 60227.888, "learn_throughput": 66.414, "synch_weights_time_ms": 2.364}, "counters": {"num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_agent_steps_sampled": 4000, "num_agent_steps_trained": 4000}, "done": false, "episodes_total": 36, "training_iteration": 1, "trial_id": "5056a_00000", "experiment_id": "aeeb2ed332fe427689bd75673aabaada", "date": "2022-12-14_11-58-21", "timestamp": 1671037101, "time_this_iter_s": 65.10799765586853, "time_total_s": 65.10799765586853, "pid": 942386, "hostname": "rhino.zoo.cs.yale.edu", "node_ip": "128.36.108.32", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a882962c0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a880ed7e0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 65.10799765586853, "timesteps_since_restore": 0, "iterations_since_restore": 1, "warmup_time": 10.01086139678955, "perf": {"cpu_util_percent": 18.61075268817204, "ram_util_percent": 27.0774193548387}}
{"custom_metrics": {}, "episode_media": {}, "num_recreated_workers": 0, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 19.827601366402, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 2.214767373441368, "policy_loss": -0.018296065810136496, "vf_loss": 2.2307312159128085, "vf_explained_var": 0.0008008929990953015, "kl": 0.011661166122125515, "entropy": 3.3737730185190835, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 128.0}}, "num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_agent_steps_sampled": 8000, "num_agent_steps_trained": 8000}, "sampler_results": {"episode_reward_max": 162.42320204734452, "episode_reward_min": 0.0, "episode_reward_mean": 59.5634625423028, "episode_len_mean": 11.0, "episode_media": {}, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [64.67200104184104, 50.103033774173554, 134.9110480429042, 17.767033253253032, 99.26977309754075, 52.62870785680384, 34.80888147576104, 21.5959399632869, 0.0, 1.4213626602602427, 84.25535810457414, 70.75241283210184, 0.0, 25.37132348564533, 129.45438713033838, 39.916526822591855, 61.33246949573479, 0.0, 125.87347864315792, 0.0, 0.0, 0.0, 0.0, 10.969366330558424, 60.77741210053516, 24.187452758881328, 20.259137366845053, 107.15011709830333, 11.50626915448768, 0.0, 97.75494214442894, 149.9892947239621, 68.02721088435374, 98.717020951884, 64.39909297052154, 0.0, 104.11191412393978, 47.70105980011699, 162.42320204734452, 29.848615865465096, 29.848615865465096, 68.88127772503549, 64.99349550855867, 86.50243245054722, 57.29445199572835, 131.1145010481888, 122.591071019773, 25.170443395171123, 104.03220516188318, 57.19741015219749, 43.83402281080649, 67.15938569729647, 88.93822695370334, 123.42402660369817, 30.234315948601658, 15.5673053266598, 74.86592520606125, 39.08747315715668, 143.74545160924748, 12.340537121878228, 119.76197454470393, 46.97043486436794, 86.81599793023113, 58.95101633429357, 70.74521214762711, 65.62242068696429, 126.4400389161209, 50.247265019177284, 67.77801840897382, 55.0277888864765, 6.910700788251808, 74.52001375935843], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.174453538454571, "mean_inference_ms": 8.206169063436398, "mean_action_processing_ms": 0.8228428413997482, "mean_env_wait_ms": 1.303932199146734, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0}, "episode_reward_max": 162.42320204734452, "episode_reward_min": 0.0, "episode_reward_mean": 59.5634625423028, "episode_len_mean": 11.0, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "hist_stats": {"episode_reward": [64.67200104184104, 50.103033774173554, 134.9110480429042, 17.767033253253032, 99.26977309754075, 52.62870785680384, 34.80888147576104, 21.5959399632869, 0.0, 1.4213626602602427, 84.25535810457414, 70.75241283210184, 0.0, 25.37132348564533, 129.45438713033838, 39.916526822591855, 61.33246949573479, 0.0, 125.87347864315792, 0.0, 0.0, 0.0, 0.0, 10.969366330558424, 60.77741210053516, 24.187452758881328, 20.259137366845053, 107.15011709830333, 11.50626915448768, 0.0, 97.75494214442894, 149.9892947239621, 68.02721088435374, 98.717020951884, 64.39909297052154, 0.0, 104.11191412393978, 47.70105980011699, 162.42320204734452, 29.848615865465096, 29.848615865465096, 68.88127772503549, 64.99349550855867, 86.50243245054722, 57.29445199572835, 131.1145010481888, 122.591071019773, 25.170443395171123, 104.03220516188318, 57.19741015219749, 43.83402281080649, 67.15938569729647, 88.93822695370334, 123.42402660369817, 30.234315948601658, 15.5673053266598, 74.86592520606125, 39.08747315715668, 143.74545160924748, 12.340537121878228, 119.76197454470393, 46.97043486436794, 86.81599793023113, 58.95101633429357, 70.74521214762711, 65.62242068696429, 126.4400389161209, 50.247265019177284, 67.77801840897382, 55.0277888864765, 6.910700788251808, 74.52001375935843], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.174453538454571, "mean_inference_ms": 8.206169063436398, "mean_action_processing_ms": 0.8228428413997482, "mean_env_wait_ms": 1.303932199146734, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "num_agent_steps_sampled": 8000, "num_agent_steps_trained": 8000, "num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "timesteps_total": 8000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 8000, "timers": {"training_iteration_time_ms": 65460.8, "load_time_ms": 0.635, "load_throughput": 6301301.784, "learn_time_ms": 60657.488, "learn_throughput": 65.944, "synch_weights_time_ms": 2.411}, "counters": {"num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_agent_steps_sampled": 8000, "num_agent_steps_trained": 8000}, "done": false, "episodes_total": 72, "training_iteration": 2, "trial_id": "5056a_00000", "experiment_id": "aeeb2ed332fe427689bd75673aabaada", "date": "2022-12-14_11-59-27", "timestamp": 1671037167, "time_this_iter_s": 65.82937240600586, "time_total_s": 130.9373700618744, "pid": 942386, "hostname": "rhino.zoo.cs.yale.edu", "node_ip": "128.36.108.32", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a78716650>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a8819b370>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 130.9373700618744, "timesteps_since_restore": 0, "iterations_since_restore": 2, "warmup_time": 10.01086139678955, "perf": {"cpu_util_percent": 17.738947368421044, "ram_util_percent": 27.168421052631558}}
{"custom_metrics": {}, "episode_media": {}, "num_recreated_workers": 0, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 19.14248881083663, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 2.8938325256429693, "policy_loss": -0.023623749713904116, "vf_loss": 2.914692208939983, "vf_explained_var": 0.002527516311214816, "kl": 0.013820304620601194, "entropy": 3.000133900744941, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 128.0}}, "num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_agent_steps_sampled": 12000, "num_agent_steps_trained": 12000}, "sampler_results": {"episode_reward_max": 244.36377778276025, "episode_reward_min": 0.0, "episode_reward_mean": 77.92727393201763, "episode_len_mean": 11.0, "episode_media": {}, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 1.4213626602602427, 84.25535810457414, 70.75241283210184, 0.0, 25.37132348564533, 129.45438713033838, 39.916526822591855, 61.33246949573479, 0.0, 125.87347864315792, 0.0, 0.0, 0.0, 0.0, 10.969366330558424, 60.77741210053516, 24.187452758881328, 20.259137366845053, 107.15011709830333, 11.50626915448768, 0.0, 97.75494214442894, 149.9892947239621, 68.02721088435374, 98.717020951884, 64.39909297052154, 0.0, 104.11191412393978, 47.70105980011699, 162.42320204734452, 29.848615865465096, 29.848615865465096, 68.88127772503549, 64.99349550855867, 86.50243245054722, 57.29445199572835, 131.1145010481888, 122.591071019773, 25.170443395171123, 104.03220516188318, 57.19741015219749, 43.83402281080649, 67.15938569729647, 88.93822695370334, 123.42402660369817, 30.234315948601658, 15.5673053266598, 74.86592520606125, 39.08747315715668, 143.74545160924748, 12.340537121878228, 119.76197454470393, 46.97043486436794, 86.81599793023113, 58.95101633429357, 70.74521214762711, 65.62242068696429, 126.4400389161209, 50.247265019177284, 67.77801840897382, 55.0277888864765, 6.910700788251808, 74.52001375935843, 76.00203814744052, 168.55408309893278, 110.13605714578273, 64.28397687161714, 147.3942028251308, 112.32318422706568, 120.23376119176001, 74.75116206581453, 137.56210333914947, 46.318149330782944, 88.73389417672163, 244.36377778276025, 104.722447401324, 164.26013117364, 0.0, 162.01047062873323, 112.64321816537594, 77.41425078557532, 94.70691694170453, 158.03722777668816, 108.0929382143864, 5.969723173093019, 135.0746518860951, 195.33424858983653, 80.97432168521819, 39.814144817214725, 42.92075288400614, 81.32464039459248, 206.80062724356642, 148.6264184858337, 36.549325549549096, 80.57019621161261, 198.57742180949546, 241.77672046608063, 0.0, 113.05732417494684], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.141650697005964, "mean_inference_ms": 8.198579870444451, "mean_action_processing_ms": 0.8230495091336222, "mean_env_wait_ms": 1.297217854568283, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0}, "episode_reward_max": 244.36377778276025, "episode_reward_min": 0.0, "episode_reward_mean": 77.92727393201763, "episode_len_mean": 11.0, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "hist_stats": {"episode_reward": [0.0, 1.4213626602602427, 84.25535810457414, 70.75241283210184, 0.0, 25.37132348564533, 129.45438713033838, 39.916526822591855, 61.33246949573479, 0.0, 125.87347864315792, 0.0, 0.0, 0.0, 0.0, 10.969366330558424, 60.77741210053516, 24.187452758881328, 20.259137366845053, 107.15011709830333, 11.50626915448768, 0.0, 97.75494214442894, 149.9892947239621, 68.02721088435374, 98.717020951884, 64.39909297052154, 0.0, 104.11191412393978, 47.70105980011699, 162.42320204734452, 29.848615865465096, 29.848615865465096, 68.88127772503549, 64.99349550855867, 86.50243245054722, 57.29445199572835, 131.1145010481888, 122.591071019773, 25.170443395171123, 104.03220516188318, 57.19741015219749, 43.83402281080649, 67.15938569729647, 88.93822695370334, 123.42402660369817, 30.234315948601658, 15.5673053266598, 74.86592520606125, 39.08747315715668, 143.74545160924748, 12.340537121878228, 119.76197454470393, 46.97043486436794, 86.81599793023113, 58.95101633429357, 70.74521214762711, 65.62242068696429, 126.4400389161209, 50.247265019177284, 67.77801840897382, 55.0277888864765, 6.910700788251808, 74.52001375935843, 76.00203814744052, 168.55408309893278, 110.13605714578273, 64.28397687161714, 147.3942028251308, 112.32318422706568, 120.23376119176001, 74.75116206581453, 137.56210333914947, 46.318149330782944, 88.73389417672163, 244.36377778276025, 104.722447401324, 164.26013117364, 0.0, 162.01047062873323, 112.64321816537594, 77.41425078557532, 94.70691694170453, 158.03722777668816, 108.0929382143864, 5.969723173093019, 135.0746518860951, 195.33424858983653, 80.97432168521819, 39.814144817214725, 42.92075288400614, 81.32464039459248, 206.80062724356642, 148.6264184858337, 36.549325549549096, 80.57019621161261, 198.57742180949546, 241.77672046608063, 0.0, 113.05732417494684], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.141650697005964, "mean_inference_ms": 8.198579870444451, "mean_action_processing_ms": 0.8230495091336222, "mean_env_wait_ms": 1.297217854568283, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "num_agent_steps_sampled": 12000, "num_agent_steps_trained": 12000, "num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "timesteps_total": 12000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 12000, "timers": {"training_iteration_time_ms": 67179.094, "load_time_ms": 0.737, "load_throughput": 5426007.762, "learn_time_ms": 62403.752, "learn_throughput": 64.099, "synch_weights_time_ms": 2.346}, "counters": {"num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_agent_steps_sampled": 12000, "num_agent_steps_trained": 12000}, "done": false, "episodes_total": 108, "training_iteration": 3, "trial_id": "5056a_00000", "experiment_id": "aeeb2ed332fe427689bd75673aabaada", "date": "2022-12-14_12-00-39", "timestamp": 1671037239, "time_this_iter_s": 70.62299418449402, "time_total_s": 201.5603642463684, "pid": 942386, "hostname": "rhino.zoo.cs.yale.edu", "node_ip": "128.36.108.32", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a880ef160>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a881bbb20>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 201.5603642463684, "timesteps_since_restore": 0, "iterations_since_restore": 3, "warmup_time": 10.01086139678955, "perf": {"cpu_util_percent": 13.99108910891089, "ram_util_percent": 23.677227722772273}}
{"custom_metrics": {}, "episode_media": {}, "num_recreated_workers": 0, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 18.655221833208557, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 4.026132098320992, "policy_loss": -0.017803477796836085, "vf_loss": 4.042038602982798, "vf_explained_var": 0.0021248032969813195, "kl": 0.009484817894170213, "entropy": 2.9137843388383105, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 128.0}}, "num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_agent_steps_sampled": 16000, "num_agent_steps_trained": 16000}, "sampler_results": {"episode_reward_max": 275.1337391146639, "episode_reward_min": 0.0, "episode_reward_mean": 109.80245768611775, "episode_len_mean": 11.0, "episode_media": {}, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [57.29445199572835, 131.1145010481888, 122.591071019773, 25.170443395171123, 104.03220516188318, 57.19741015219749, 43.83402281080649, 67.15938569729647, 88.93822695370334, 123.42402660369817, 30.234315948601658, 15.5673053266598, 74.86592520606125, 39.08747315715668, 143.74545160924748, 12.340537121878228, 119.76197454470393, 46.97043486436794, 86.81599793023113, 58.95101633429357, 70.74521214762711, 65.62242068696429, 126.4400389161209, 50.247265019177284, 67.77801840897382, 55.0277888864765, 6.910700788251808, 74.52001375935843, 76.00203814744052, 168.55408309893278, 110.13605714578273, 64.28397687161714, 147.3942028251308, 112.32318422706568, 120.23376119176001, 74.75116206581453, 137.56210333914947, 46.318149330782944, 88.73389417672163, 244.36377778276025, 104.722447401324, 164.26013117364, 0.0, 162.01047062873323, 112.64321816537594, 77.41425078557532, 94.70691694170453, 158.03722777668816, 108.0929382143864, 5.969723173093019, 135.0746518860951, 195.33424858983653, 80.97432168521819, 39.814144817214725, 42.92075288400614, 81.32464039459248, 206.80062724356642, 148.6264184858337, 36.549325549549096, 80.57019621161261, 198.57742180949546, 241.77672046608063, 0.0, 113.05732417494684, 84.77006905792086, 153.50650424558117, 66.63703491965083, 50.312539729555795, 197.77599166918026, 127.12685820547748, 134.81842886594373, 107.88635256761206, 104.13207765199132, 159.8393379595656, 131.77537828192965, 150.05811416946656, 275.1337391146639, 253.77293208818426, 149.68343066374086, 257.3389067709614, 179.78736992663102, 78.05049804020918, 94.77118943869664, 97.33736851730878, 113.25899501974459, 172.60728349480823, 146.29301587877282, 20.339022828962044, 195.44792447983104, 263.289259293603, 133.59742332168253, 97.40405733849722, 202.2989940281897, 52.55781592360927, 148.0091366590955, 115.30713664645324, 129.7239500898029, 140.808897638388, 114.50533480463689, 133.98125512530075], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.08326150269259, "mean_inference_ms": 8.152776058777276, "mean_action_processing_ms": 0.8228307736980482, "mean_env_wait_ms": 1.2909288382816875, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0}, "episode_reward_max": 275.1337391146639, "episode_reward_min": 0.0, "episode_reward_mean": 109.80245768611775, "episode_len_mean": 11.0, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "hist_stats": {"episode_reward": [57.29445199572835, 131.1145010481888, 122.591071019773, 25.170443395171123, 104.03220516188318, 57.19741015219749, 43.83402281080649, 67.15938569729647, 88.93822695370334, 123.42402660369817, 30.234315948601658, 15.5673053266598, 74.86592520606125, 39.08747315715668, 143.74545160924748, 12.340537121878228, 119.76197454470393, 46.97043486436794, 86.81599793023113, 58.95101633429357, 70.74521214762711, 65.62242068696429, 126.4400389161209, 50.247265019177284, 67.77801840897382, 55.0277888864765, 6.910700788251808, 74.52001375935843, 76.00203814744052, 168.55408309893278, 110.13605714578273, 64.28397687161714, 147.3942028251308, 112.32318422706568, 120.23376119176001, 74.75116206581453, 137.56210333914947, 46.318149330782944, 88.73389417672163, 244.36377778276025, 104.722447401324, 164.26013117364, 0.0, 162.01047062873323, 112.64321816537594, 77.41425078557532, 94.70691694170453, 158.03722777668816, 108.0929382143864, 5.969723173093019, 135.0746518860951, 195.33424858983653, 80.97432168521819, 39.814144817214725, 42.92075288400614, 81.32464039459248, 206.80062724356642, 148.6264184858337, 36.549325549549096, 80.57019621161261, 198.57742180949546, 241.77672046608063, 0.0, 113.05732417494684, 84.77006905792086, 153.50650424558117, 66.63703491965083, 50.312539729555795, 197.77599166918026, 127.12685820547748, 134.81842886594373, 107.88635256761206, 104.13207765199132, 159.8393379595656, 131.77537828192965, 150.05811416946656, 275.1337391146639, 253.77293208818426, 149.68343066374086, 257.3389067709614, 179.78736992663102, 78.05049804020918, 94.77118943869664, 97.33736851730878, 113.25899501974459, 172.60728349480823, 146.29301587877282, 20.339022828962044, 195.44792447983104, 263.289259293603, 133.59742332168253, 97.40405733849722, 202.2989940281897, 52.55781592360927, 148.0091366590955, 115.30713664645324, 129.7239500898029, 140.808897638388, 114.50533480463689, 133.98125512530075], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.08326150269259, "mean_inference_ms": 8.152776058777276, "mean_action_processing_ms": 0.8228307736980482, "mean_env_wait_ms": 1.2909288382816875, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "num_agent_steps_sampled": 16000, "num_agent_steps_trained": 16000, "num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "timesteps_total": 16000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 16000, "timers": {"training_iteration_time_ms": 69231.164, "load_time_ms": 0.794, "load_throughput": 5039716.431, "learn_time_ms": 64496.756, "learn_throughput": 62.019, "synch_weights_time_ms": 2.328}, "counters": {"num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_agent_steps_sampled": 16000, "num_agent_steps_trained": 16000}, "done": false, "episodes_total": 144, "training_iteration": 4, "trial_id": "5056a_00000", "experiment_id": "aeeb2ed332fe427689bd75673aabaada", "date": "2022-12-14_12-01-55", "timestamp": 1671037315, "time_this_iter_s": 75.39499735832214, "time_total_s": 276.95536160469055, "pid": 942386, "hostname": "rhino.zoo.cs.yale.edu", "node_ip": "128.36.108.32", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a881bca30>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a7875fa30>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 276.95536160469055, "timesteps_since_restore": 0, "iterations_since_restore": 4, "warmup_time": 10.01086139678955, "perf": {"cpu_util_percent": 11.133027522935778, "ram_util_percent": 22.631192660550447}}
{"custom_metrics": {}, "episode_media": {}, "num_recreated_workers": 0, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 14.32508194344018, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 3.5730534484309535, "policy_loss": -0.009664036417680402, "vf_loss": 3.5810282625177856, "vf_explained_var": 0.007720269054494878, "kl": 0.00844608764012274, "entropy": 2.7788806488437037, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 128.0}}, "num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_agent_steps_sampled": 20000, "num_agent_steps_trained": 20000}, "sampler_results": {"episode_reward_max": 275.1337391146639, "episode_reward_min": 0.0, "episode_reward_mean": 133.38342740799152, "episode_len_mean": 11.0, "episode_media": {}, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [137.56210333914947, 46.318149330782944, 88.73389417672163, 244.36377778276025, 104.722447401324, 164.26013117364, 0.0, 162.01047062873323, 112.64321816537594, 77.41425078557532, 94.70691694170453, 158.03722777668816, 108.0929382143864, 5.969723173093019, 135.0746518860951, 195.33424858983653, 80.97432168521819, 39.814144817214725, 42.92075288400614, 81.32464039459248, 206.80062724356642, 148.6264184858337, 36.549325549549096, 80.57019621161261, 198.57742180949546, 241.77672046608063, 0.0, 113.05732417494684, 84.77006905792086, 153.50650424558117, 66.63703491965083, 50.312539729555795, 197.77599166918026, 127.12685820547748, 134.81842886594373, 107.88635256761206, 104.13207765199132, 159.8393379595656, 131.77537828192965, 150.05811416946656, 275.1337391146639, 253.77293208818426, 149.68343066374086, 257.3389067709614, 179.78736992663102, 78.05049804020918, 94.77118943869664, 97.33736851730878, 113.25899501974459, 172.60728349480823, 146.29301587877282, 20.339022828962044, 195.44792447983104, 263.289259293603, 133.59742332168253, 97.40405733849722, 202.2989940281897, 52.55781592360927, 148.0091366590955, 115.30713664645324, 129.7239500898029, 140.808897638388, 114.50533480463689, 133.98125512530075, 80.23568044334712, 257.57222601856785, 188.0443921691755, 190.77381788986847, 67.60686595508456, 102.02144970506475, 163.73920175865464, 259.7980799445354, 106.1868098354638, 98.58225605740196, 237.38648101714796, 27.609969675555213, 142.3746530795768, 173.8885306694092, 218.44416597755483, 124.72051240166414, 186.1756045220251, 133.03954500035871, 165.28207256316654, 123.40066295497016, 94.24988116249466, 141.40782354469727, 215.64265918007413, 178.21958936691505, 129.47769900891285, 137.9574638336649, 158.94420944279065, 183.06546096669655, 75.92392982750088, 82.78777577637935, 94.53610087678635, 124.49385506087488, 186.83812169120887, 130.85168804576216, 182.19582052819717, 34.688017303970206], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.043134949347301, "mean_inference_ms": 8.105135865615058, "mean_action_processing_ms": 0.8223834591329038, "mean_env_wait_ms": 1.2805606744572193, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0}, "episode_reward_max": 275.1337391146639, "episode_reward_min": 0.0, "episode_reward_mean": 133.38342740799152, "episode_len_mean": 11.0, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "hist_stats": {"episode_reward": [137.56210333914947, 46.318149330782944, 88.73389417672163, 244.36377778276025, 104.722447401324, 164.26013117364, 0.0, 162.01047062873323, 112.64321816537594, 77.41425078557532, 94.70691694170453, 158.03722777668816, 108.0929382143864, 5.969723173093019, 135.0746518860951, 195.33424858983653, 80.97432168521819, 39.814144817214725, 42.92075288400614, 81.32464039459248, 206.80062724356642, 148.6264184858337, 36.549325549549096, 80.57019621161261, 198.57742180949546, 241.77672046608063, 0.0, 113.05732417494684, 84.77006905792086, 153.50650424558117, 66.63703491965083, 50.312539729555795, 197.77599166918026, 127.12685820547748, 134.81842886594373, 107.88635256761206, 104.13207765199132, 159.8393379595656, 131.77537828192965, 150.05811416946656, 275.1337391146639, 253.77293208818426, 149.68343066374086, 257.3389067709614, 179.78736992663102, 78.05049804020918, 94.77118943869664, 97.33736851730878, 113.25899501974459, 172.60728349480823, 146.29301587877282, 20.339022828962044, 195.44792447983104, 263.289259293603, 133.59742332168253, 97.40405733849722, 202.2989940281897, 52.55781592360927, 148.0091366590955, 115.30713664645324, 129.7239500898029, 140.808897638388, 114.50533480463689, 133.98125512530075, 80.23568044334712, 257.57222601856785, 188.0443921691755, 190.77381788986847, 67.60686595508456, 102.02144970506475, 163.73920175865464, 259.7980799445354, 106.1868098354638, 98.58225605740196, 237.38648101714796, 27.609969675555213, 142.3746530795768, 173.8885306694092, 218.44416597755483, 124.72051240166414, 186.1756045220251, 133.03954500035871, 165.28207256316654, 123.40066295497016, 94.24988116249466, 141.40782354469727, 215.64265918007413, 178.21958936691505, 129.47769900891285, 137.9574638336649, 158.94420944279065, 183.06546096669655, 75.92392982750088, 82.78777577637935, 94.53610087678635, 124.49385506087488, 186.83812169120887, 130.85168804576216, 182.19582052819717, 34.688017303970206], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.043134949347301, "mean_inference_ms": 8.105135865615058, "mean_action_processing_ms": 0.8223834591329038, "mean_env_wait_ms": 1.2805606744572193, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "num_agent_steps_sampled": 20000, "num_agent_steps_trained": 20000, "num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "timesteps_total": 20000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 20000, "timers": {"training_iteration_time_ms": 69843.447, "load_time_ms": 0.828, "load_throughput": 4828253.712, "learn_time_ms": 65127.446, "learn_throughput": 61.418, "synch_weights_time_ms": 2.3}, "counters": {"num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_agent_steps_sampled": 20000, "num_agent_steps_trained": 20000}, "done": false, "episodes_total": 180, "training_iteration": 5, "trial_id": "5056a_00000", "experiment_id": "aeeb2ed332fe427689bd75673aabaada", "date": "2022-12-14_12-03-08", "timestamp": 1671037388, "time_this_iter_s": 72.30011820793152, "time_total_s": 349.25547981262207, "pid": 942386, "hostname": "rhino.zoo.cs.yale.edu", "node_ip": "128.36.108.32", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a787de110>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a7859c0a0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 349.25547981262207, "timesteps_since_restore": 0, "iterations_since_restore": 5, "warmup_time": 10.01086139678955, "perf": {"cpu_util_percent": 11.274038461538462, "ram_util_percent": 22.675961538461532}}
{"custom_metrics": {}, "episode_media": {}, "num_recreated_workers": 0, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 841.0645417831278, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 3.860967412174389, "policy_loss": 0.03782374855100868, "vf_loss": 3.7872585799104423, "vf_explained_var": 0.010577162299104916, "kl": 0.17942539419375847, "entropy": 2.7122062626705374, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 128.0}}, "num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_agent_steps_sampled": 24000, "num_agent_steps_trained": 24000}, "sampler_results": {"episode_reward_max": 275.1337391146639, "episode_reward_min": 20.339022828962044, "episode_reward_mean": 143.3022697016849, "episode_len_mean": 11.0, "episode_media": {}, "episodes_this_iter": 38, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [131.77537828192965, 150.05811416946656, 275.1337391146639, 253.77293208818426, 149.68343066374086, 257.3389067709614, 179.78736992663102, 78.05049804020918, 94.77118943869664, 97.33736851730878, 113.25899501974459, 172.60728349480823, 146.29301587877282, 20.339022828962044, 195.44792447983104, 263.289259293603, 133.59742332168253, 97.40405733849722, 202.2989940281897, 52.55781592360927, 148.0091366590955, 115.30713664645324, 129.7239500898029, 140.808897638388, 114.50533480463689, 133.98125512530075, 80.23568044334712, 257.57222601856785, 188.0443921691755, 190.77381788986847, 67.60686595508456, 102.02144970506475, 163.73920175865464, 259.7980799445354, 106.1868098354638, 98.58225605740196, 237.38648101714796, 27.609969675555213, 142.3746530795768, 173.8885306694092, 218.44416597755483, 124.72051240166414, 186.1756045220251, 133.03954500035871, 165.28207256316654, 123.40066295497016, 94.24988116249466, 141.40782354469727, 215.64265918007413, 178.21958936691505, 129.47769900891285, 137.9574638336649, 158.94420944279065, 183.06546096669655, 75.92392982750088, 82.78777577637935, 94.53610087678635, 124.49385506087488, 186.83812169120887, 130.85168804576216, 182.19582052819717, 34.688017303970206, 138.94908409891838, 81.97785287479178, 152.24149716024425, 139.80345455987216, 53.23702256392078, 147.7332215788372, 179.38298570297772, 106.55436777885757, 108.96225323953256, 94.10925234463527, 91.69499568005664, 100.32441697797313, 112.61080584488599, 98.07033245679841, 162.69230047543675, 88.69009420950636, 152.51793495437636, 174.5071172383592, 236.29527727392056, 135.335045696679, 217.851025558092, 202.94156839751568, 69.07822528864779, 200.95879117583036, 148.47918795348104, 92.22224133579564, 162.04902388527267, 112.04997442287367, 200.95187274595958, 171.90670694517507, 100.69658195781398, 86.89636444614045, 147.20163273064406, 227.8587391435162, 156.2281461483818, 148.20409253703306, 80.09220908146345, 203.56977086558453], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.011394048440234, "mean_inference_ms": 8.055891878199711, "mean_action_processing_ms": 0.8216600759411614, "mean_env_wait_ms": 1.268770764513342, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0}, "episode_reward_max": 275.1337391146639, "episode_reward_min": 20.339022828962044, "episode_reward_mean": 143.3022697016849, "episode_len_mean": 11.0, "episodes_this_iter": 38, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "hist_stats": {"episode_reward": [131.77537828192965, 150.05811416946656, 275.1337391146639, 253.77293208818426, 149.68343066374086, 257.3389067709614, 179.78736992663102, 78.05049804020918, 94.77118943869664, 97.33736851730878, 113.25899501974459, 172.60728349480823, 146.29301587877282, 20.339022828962044, 195.44792447983104, 263.289259293603, 133.59742332168253, 97.40405733849722, 202.2989940281897, 52.55781592360927, 148.0091366590955, 115.30713664645324, 129.7239500898029, 140.808897638388, 114.50533480463689, 133.98125512530075, 80.23568044334712, 257.57222601856785, 188.0443921691755, 190.77381788986847, 67.60686595508456, 102.02144970506475, 163.73920175865464, 259.7980799445354, 106.1868098354638, 98.58225605740196, 237.38648101714796, 27.609969675555213, 142.3746530795768, 173.8885306694092, 218.44416597755483, 124.72051240166414, 186.1756045220251, 133.03954500035871, 165.28207256316654, 123.40066295497016, 94.24988116249466, 141.40782354469727, 215.64265918007413, 178.21958936691505, 129.47769900891285, 137.9574638336649, 158.94420944279065, 183.06546096669655, 75.92392982750088, 82.78777577637935, 94.53610087678635, 124.49385506087488, 186.83812169120887, 130.85168804576216, 182.19582052819717, 34.688017303970206, 138.94908409891838, 81.97785287479178, 152.24149716024425, 139.80345455987216, 53.23702256392078, 147.7332215788372, 179.38298570297772, 106.55436777885757, 108.96225323953256, 94.10925234463527, 91.69499568005664, 100.32441697797313, 112.61080584488599, 98.07033245679841, 162.69230047543675, 88.69009420950636, 152.51793495437636, 174.5071172383592, 236.29527727392056, 135.335045696679, 217.851025558092, 202.94156839751568, 69.07822528864779, 200.95879117583036, 148.47918795348104, 92.22224133579564, 162.04902388527267, 112.04997442287367, 200.95187274595958, 171.90670694517507, 100.69658195781398, 86.89636444614045, 147.20163273064406, 227.8587391435162, 156.2281461483818, 148.20409253703306, 80.09220908146345, 203.56977086558453], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 13.011394048440234, "mean_inference_ms": 8.055891878199711, "mean_action_processing_ms": 0.8216600759411614, "mean_env_wait_ms": 1.268770764513342, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "num_agent_steps_sampled": 24000, "num_agent_steps_trained": 24000, "num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "timesteps_total": 24000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 24000, "timers": {"training_iteration_time_ms": 70961.358, "load_time_ms": 0.847, "load_throughput": 4720214.574, "learn_time_ms": 66261.655, "learn_throughput": 60.367, "synch_weights_time_ms": 2.29}, "counters": {"num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_agent_steps_sampled": 24000, "num_agent_steps_trained": 24000}, "done": false, "episodes_total": 218, "training_iteration": 6, "trial_id": "5056a_00000", "experiment_id": "aeeb2ed332fe427689bd75673aabaada", "date": "2022-12-14_12-04-26", "timestamp": 1671037466, "time_this_iter_s": 76.55865716934204, "time_total_s": 425.8141369819641, "pid": 942386, "hostname": "rhino.zoo.cs.yale.edu", "node_ip": "128.36.108.32", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a787c6e00>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a783df130>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 425.8141369819641, "timesteps_since_restore": 0, "iterations_since_restore": 6, "warmup_time": 10.01086139678955, "perf": {"cpu_util_percent": 11.142727272727274, "ram_util_percent": 22.66999999999999}}
{"custom_metrics": {}, "episode_media": {}, "num_recreated_workers": 0, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 95.49756798103292, "cur_kl_coeff": 0.3, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.3057234409355347, "policy_loss": 0.02012070896784945, "vf_loss": 1.2391933884812139, "vf_explained_var": -0.03604278000452185, "kl": 0.154697794135654, "entropy": 3.5838531709486436, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 128.0}}, "num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_agent_steps_sampled": 28000, "num_agent_steps_trained": 28000}, "sampler_results": {"episode_reward_max": 237.38648101714796, "episode_reward_min": 0.0, "episode_reward_mean": 108.39320728566273, "episode_len_mean": 11.0, "episode_media": {}, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [237.38648101714796, 27.609969675555213, 142.3746530795768, 173.8885306694092, 218.44416597755483, 124.72051240166414, 186.1756045220251, 133.03954500035871, 165.28207256316654, 123.40066295497016, 94.24988116249466, 141.40782354469727, 215.64265918007413, 178.21958936691505, 129.47769900891285, 137.9574638336649, 158.94420944279065, 183.06546096669655, 75.92392982750088, 82.78777577637935, 94.53610087678635, 124.49385506087488, 186.83812169120887, 130.85168804576216, 182.19582052819717, 34.688017303970206, 138.94908409891838, 81.97785287479178, 152.24149716024425, 139.80345455987216, 53.23702256392078, 147.7332215788372, 179.38298570297772, 106.55436777885757, 108.96225323953256, 94.10925234463527, 91.69499568005664, 100.32441697797313, 112.61080584488599, 98.07033245679841, 162.69230047543675, 88.69009420950636, 152.51793495437636, 174.5071172383592, 236.29527727392056, 135.335045696679, 217.851025558092, 202.94156839751568, 69.07822528864779, 200.95879117583036, 148.47918795348104, 92.22224133579564, 162.04902388527267, 112.04997442287367, 200.95187274595958, 171.90670694517507, 100.69658195781398, 86.89636444614045, 147.20163273064406, 227.8587391435162, 156.2281461483818, 148.20409253703306, 80.09220908146345, 203.56977086558453, 16.115222905444927, 125.8024777604809, 8.54881956210333, 77.6454842816081, 0.0, 104.05829647515867, 70.5173549821613, 46.19428645845788, 76.32451400245589, 14.685239175035091, 87.98185941043084, 0.0, 23.582766439909296, 47.05645088389919, 56.440048057317455, 65.03267181688209, 62.196307094266274, 0.0, 73.12910887038949, 0.0, 50.102580714825606, 46.647230320699705, 132.1416879749056, 21.93873266111685, 135.44973544973544, 41.78806221165114, 123.53053721839152, 4.264087980780728, 84.65608465608464, 81.3740783218941, 34.82338517637594, 56.48001932574984, 0.0, 32.87505472710765, 69.41078284279638, 0.0], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 12.987165251557434, "mean_inference_ms": 8.020991006845462, "mean_action_processing_ms": 0.8210421437495325, "mean_env_wait_ms": 1.265949999658731, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0}, "episode_reward_max": 237.38648101714796, "episode_reward_min": 0.0, "episode_reward_mean": 108.39320728566273, "episode_len_mean": 11.0, "episodes_this_iter": 36, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "hist_stats": {"episode_reward": [237.38648101714796, 27.609969675555213, 142.3746530795768, 173.8885306694092, 218.44416597755483, 124.72051240166414, 186.1756045220251, 133.03954500035871, 165.28207256316654, 123.40066295497016, 94.24988116249466, 141.40782354469727, 215.64265918007413, 178.21958936691505, 129.47769900891285, 137.9574638336649, 158.94420944279065, 183.06546096669655, 75.92392982750088, 82.78777577637935, 94.53610087678635, 124.49385506087488, 186.83812169120887, 130.85168804576216, 182.19582052819717, 34.688017303970206, 138.94908409891838, 81.97785287479178, 152.24149716024425, 139.80345455987216, 53.23702256392078, 147.7332215788372, 179.38298570297772, 106.55436777885757, 108.96225323953256, 94.10925234463527, 91.69499568005664, 100.32441697797313, 112.61080584488599, 98.07033245679841, 162.69230047543675, 88.69009420950636, 152.51793495437636, 174.5071172383592, 236.29527727392056, 135.335045696679, 217.851025558092, 202.94156839751568, 69.07822528864779, 200.95879117583036, 148.47918795348104, 92.22224133579564, 162.04902388527267, 112.04997442287367, 200.95187274595958, 171.90670694517507, 100.69658195781398, 86.89636444614045, 147.20163273064406, 227.8587391435162, 156.2281461483818, 148.20409253703306, 80.09220908146345, 203.56977086558453, 16.115222905444927, 125.8024777604809, 8.54881956210333, 77.6454842816081, 0.0, 104.05829647515867, 70.5173549821613, 46.19428645845788, 76.32451400245589, 14.685239175035091, 87.98185941043084, 0.0, 23.582766439909296, 47.05645088389919, 56.440048057317455, 65.03267181688209, 62.196307094266274, 0.0, 73.12910887038949, 0.0, 50.102580714825606, 46.647230320699705, 132.1416879749056, 21.93873266111685, 135.44973544973544, 41.78806221165114, 123.53053721839152, 4.264087980780728, 84.65608465608464, 81.3740783218941, 34.82338517637594, 56.48001932574984, 0.0, 32.87505472710765, 69.41078284279638, 0.0], "episode_lengths": [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, "sampler_perf": {"mean_raw_obs_processing_ms": 12.987165251557434, "mean_inference_ms": 8.020991006845462, "mean_action_processing_ms": 0.8210421437495325, "mean_env_wait_ms": 1.265949999658731, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "num_agent_steps_sampled": 28000, "num_agent_steps_trained": 28000, "num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "timesteps_total": 28000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 28000, "timers": {"training_iteration_time_ms": 71157.075, "load_time_ms": 0.862, "load_throughput": 4641550.549, "learn_time_ms": 66474.188, "learn_throughput": 60.174, "synch_weights_time_ms": 2.277}, "counters": {"num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_agent_steps_sampled": 28000, "num_agent_steps_trained": 28000}, "done": false, "episodes_total": 254, "training_iteration": 7, "trial_id": "5056a_00000", "experiment_id": "aeeb2ed332fe427689bd75673aabaada", "date": "2022-12-14_12-05-39", "timestamp": 1671037539, "time_this_iter_s": 72.3384702205658, "time_total_s": 498.1526072025299, "pid": 942386, "hostname": "rhino.zoo.cs.yale.edu", "node_ip": "128.36.108.32", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "job_search_env", "env_config": {}, "observation_space": "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))", "action_space": "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))", "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 1, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 5e-05, "train_batch_size": 4000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "<class 'models.job_search_model.JobSearchModelV0'>", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a7859d480>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "logger_creator": null, "logger_config": null, "log_level": "ERROR", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": true, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "replay_mode": -1, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "kl_coeff": 0.2, "sgd_minibatch_size": 128, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f6a78207c70>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 498.1526072025299, "timesteps_since_restore": 0, "iterations_since_restore": 7, "warmup_time": 10.01086139678955, "perf": {"cpu_util_percent": 10.84423076923077, "ram_util_percent": 22.682692307692303}}
