{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f091c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import environment.job_search_environment as job_search_env\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from supersuit import pad_observations_v0, pad_action_space_v0\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45796123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is working on the Zoo, but not on my local machine (M1 compatibility issues)\n",
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7a3a74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.rllib.models.modelv2 import restore_original_dimensions\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.tune.registry import register_env\n",
    "import torch\n",
    "from torch import nn\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99df7597",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, _ = try_import_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53182e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(args):\n",
    "    env = job_search_env.env()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a3bd1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('candidate_obs': Dict('accepted_offer': Dict('employer_0': Discrete(101)), 'counter_offers': Dict('employer_0': Tuple(Discrete(101), Discrete(11))), 'current_offers': Dict('employer_0': Tuple(Discrete(101), Discrete(11))), 'job_openings': Dict('employer_0': Discrete(2)), 'rejected_offers': Dict('employer_0': Tuple(Discrete(2), Discrete(101)))), 'employer_obs': Dict('accepted_offers': Dict('candidate_0': Discrete(2)), 'candidate_strengths': Dict('candidate_0': Discrete(101)), 'counter_offers': Dict('candidate_0': Tuple(Discrete(101), Discrete(11))), 'declined_offers': Dict('candidate_0': Tuple(Discrete(2), Discrete(101))), 'job_applicants': Dict('candidate_0': Discrete(2)), 'outstanding_offers': Dict('candidate_0': Tuple(Discrete(101), Discrete(11))), 'rejected_offers': Dict('candidate_0': Tuple(Discrete(2), Discrete(101))), 'remaining_budget': Discrete(101)))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_search_env.env().observation_space(agent=\"candidate_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c791842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"job_search\"\n",
    "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "162f4fc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ray\u001b[38;5;241m.\u001b[39minit()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/worker.py:1364\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RayContext(\u001b[38;5;28mdict\u001b[39m(_global_node\u001b[38;5;241m.\u001b[39maddress_info, node_id\u001b[38;5;241m=\u001b[39mnode_id\u001b[38;5;241m.\u001b[39mhex()))\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1364\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1365\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe you called ray.init twice by accident? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis error can be suppressed by passing in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_reinit_error=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mray.shutdown()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m prior to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mray.init()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1371\u001b[0m _system_config \u001b[38;5;241m=\u001b[39m _system_config \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_system_config, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf70d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to deal with the Dictionary space, need to pass a custom model to RLlib.\n",
    "See: https://medium.com/@nima.siboni/rllib-with-dictionary-state-baa06b64470f\n",
    "\"\"\"\n",
    "class CandidateModelV0(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
    "        TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
    "        nn.Module.__init__(self)\n",
    "#         self.layer1 = torch.cat()\n",
    "        self.next_layers = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.policy_fn = nn.Linear(512, num_outputs)\n",
    "        self.value_fn = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        model_out = self.model(input_dict[\"obs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c52ca8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"CandidateModelV0\", CandidateModelV0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b40f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use policy_map to map different policies to candidate and employer agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b2d7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=env_name, clip_actions=True)\n",
    "    .debugging(log_level=\"ERROR\")\n",
    "    .framework(framework=\"torch\")\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    ").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93b778a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'job_search', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': True, 'disable_env_checking': False, 'num_workers': 2, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'ERROR', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'replay_mode': -1, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}\n"
     ]
    }
   ],
   "source": [
    "print (config)\n",
    "config[\"observation_space\"] = None\n",
    "config[\"action_space\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8afa733c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(...)? (3875903284.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [27]\u001b[0;36m\u001b[0m\n\u001b[0;31m    print config.to_dict()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(...)?\n"
     ]
    }
   ],
   "source": [
    "tune.run(\n",
    "    \"PPO\",\n",
    "    name=\"PPO\",\n",
    "    stop={\"timesteps_total\": 5000000},\n",
    "    checkpoint_freq=10,\n",
    "    local_dir=\"~/ray_results/\" + env_name,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a39880",
   "metadata": {},
   "source": [
    "# Run 1\n",
    "Got the error: \n",
    "```\n",
    "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
    "```\n",
    "\n",
    "# Run 2\n",
    "\n",
    "```\n",
    "AssertionError: homogenization only supports Discrete and Box spaces\n",
    "```\n",
    "\n",
    "# Run 3\n",
    "\n",
    "Same error\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
    "    future_result = ray.get(ready_future)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
    "    return func(*args, **kwargs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2291, in get\n",
    "    raise value\n",
    "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::PPO.__init__() (pid=1879526, ip=128.36.108.57, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 139, in __init__\n",
    "    self.add_workers(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 490, in add_workers\n",
    "    self.foreach_worker(lambda w: w.assert_healthy())\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 620, in foreach_worker\n",
    "    remote_results = ray.get([w.apply.remote(func) for w in self.remote_workers()])\n",
    "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=1879573, ip=128.36.108.57, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa145f35d50>)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 492, in __init__\n",
    "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
    "  File \"/tmp/ipykernel_1861406/2067092441.py\", line 2, in <lambda>\n",
    "  File \"/tmp/ipykernel_1861406/841127696.py\", line 3, in env_creator\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/supersuit/multiagent_wrappers/padding_wrappers.py\", line 33, in pad_observations_v0\n",
    "    homogenize_ops.check_homogenize_spaces(spaces)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/supersuit/utils/action_transforms/homogenize_ops.py\", line 30, in check_homogenize_spaces\n",
    "    assert False, \"homogenization only supports Discrete and Box spaces\"\n",
    "AssertionError: homogenization only supports Discrete and Box spaces\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "ray::PPO.__init__() (pid=1879526, ip=128.36.108.57, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 414, in __init__\n",
    "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
    "    self.setup(copy.deepcopy(self.config))\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 549, in setup\n",
    "    raise e.args[0].args[2]\n",
    "AssertionError: homogenization only supports Discrete and Box spaces\n",
    "```\n",
    "\n",
    "i.e. I cannot use the SuperSuit wrapper to fix the issue of observation spaces for all agents needing to be identical.\n",
    "\n",
    "# Run 4\n",
    "\n",
    "Fixed the issue by making all observation spaces and action spaces the same for all agents.\n",
    "\n",
    "New issue:\n",
    "\n",
    "```\n",
    "Failure # 1 (occurred at 2022-12-12_17-37-17)\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
    "    future_result = ray.get(ready_future)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
    "    return func(*args, **kwargs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2291, in get\n",
    "    raise value\n",
    "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::PPO.__init__() (pid=499392, ip=128.36.232.21, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 139, in __init__\n",
    "    self.add_workers(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 490, in add_workers\n",
    "    self.foreach_worker(lambda w: w.assert_healthy())\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 620, in foreach_worker\n",
    "    remote_results = ray.get([w.apply.remote(func) for w in self.remote_workers()])\n",
    "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=499452, ip=128.36.232.21, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8059b28df0>)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 567, in __init__\n",
    "    self.policy_dict = _determine_spaces_for_multi_agent_dict(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2121, in _determine_spaces_for_multi_agent_dict\n",
    "    raise ValueError(\n",
    "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "ray::PPO.__init__() (pid=499392, ip=128.36.232.21, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 414, in __init__\n",
    "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
    "    self.setup(copy.deepcopy(self.config))\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 549, in setup\n",
    "    raise e.args[0].args[2]\n",
    "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7130b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
