{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f091c0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import environment.job_search_environment as job_search_env\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from supersuit import pad_observations_v0, pad_action_space_v0\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe3b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Dict, Tuple, Box\n",
    "\n",
    "from gym.spaces.utils import flatten, flatdim, flatten_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45796123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is working on the Zoo, but not on my local machine (M1 compatibility issues)\n",
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83500be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20e8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a3a74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.rllib.models.modelv2 import restore_original_dimensions\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.tune.registry import register_env\n",
    "import torch\n",
    "from torch import nn\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.torch_utils import FLOAT_MIN\n",
    "from ray.rllib.models.preprocessors import Preprocessor, DictFlatteningPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99df7597",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, _ = try_import_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624fb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162f4fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1214 02:30:46.594696686  720893 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E1214 02:30:46.630197916  720893 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E1214 02:30:49.883287415  720893 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E1214 02:30:49.925284587  720893 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "2022-12-14 02:30:50,371\tINFO worker.py:1528 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.8</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.1.0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.10.8', ray_version='2.1.0', ray_commit='be49bde7ee4f6adb3f8710aee0665c27f9f0bb62', address_info={'node_ip_address': '128.36.108.32', 'raylet_ip_address': '128.36.108.32', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-12-14_02-30-45_537596_720893/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-12-14_02-30-45_537596_720893/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-12-14_02-30-45_537596_720893', 'metrics_export_port': 57474, 'gcs_address': '128.36.108.32:39613', 'address': '128.36.108.32:39613', 'dashboard_agent_listen_port': 52365, 'node_id': 'd7a87063a04bf23d899fe9e9a3148fe0fc3854ad54fb8f8ca8549163'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf70d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to deal with the Dictionary space, need to pass a custom model to RLlib.\n",
    "See: https://medium.com/@nima.siboni/rllib-with-dictionary-state-baa06b64470f\n",
    "\"\"\"\n",
    "class CandidateModelV0(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, **kwargs):\n",
    "        orig_space = getattr(obs_space, \"original_space\", obs_space)\n",
    "        assert (\n",
    "            isinstance(orig_space, Dict)\n",
    "            and \"action_mask\" in orig_space.spaces\n",
    "            and \"observation\" in orig_space.spaces\n",
    "        )\n",
    "        print(\"Orig space\")\n",
    "        print(orig_space)\n",
    "        print(\"Obs space\")\n",
    "        print(obs_space)\n",
    "        print(\"Flattened obs space\")\n",
    "        print(flatten_space(orig_space[\"observation\"]))\n",
    "        print(\"Act space\")\n",
    "        print(action_space)\n",
    "        print(\"Num outputs\")\n",
    "        print(num_outputs)\n",
    "        print(\"Model config\") \n",
    "        print(model_config)\n",
    "        \n",
    "#         self.orig_space = orig_space\n",
    "        \n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name, **kwargs)\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        self.internal_model = TorchFC(\n",
    "            flatten_space(orig_space[\"observation\"]),\n",
    "            action_space,\n",
    "            num_outputs,\n",
    "            model_config,\n",
    "            name + \"_internal\",\n",
    "        )\n",
    "        # disable action masking --> will likely lead to invalid actions\n",
    "        self.no_masking = False\n",
    "        if \"no_masking\" in model_config[\"custom_model_config\"]:\n",
    "            self.no_masking = model_config[\"custom_model_config\"][\"no_masking\"]\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Extract the available actions tensor from the observation.\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "        \n",
    "#         print(\"original_model\")\n",
    "        \n",
    "#         print(input_dict[\"obs_flat\"][:,self.num_outputs:])\n",
    "#         print(input_dict[\"obs_flat\"][:,self.num_outputs:].size())\n",
    "        \n",
    "        # Compute the unmasked logits.\n",
    "        logits, _ = self.internal_model({\"obs\": input_dict[\"obs_flat\"][:,self.num_outputs:]})\n",
    "        \n",
    "        \n",
    "#         print(\"logits:\\n\", logits)\n",
    "        \n",
    "        \n",
    "        # If action masking is disabled, directly return unmasked logits\n",
    "        if self.no_masking:\n",
    "            return logits, state\n",
    "\n",
    "        # Convert action_mask into a [0.0 || -inf]-type mask.\n",
    "        inf_mask = torch.clamp(torch.log(action_mask), min=FLOAT_MIN)\n",
    "        masked_logits = logits + inf_mask\n",
    "\n",
    "#         print(\"masks:\\n\", inf_mask, \"\\n\", masked_logits)\n",
    "        \n",
    "        # Return masked logits.\n",
    "        return masked_logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.internal_model.value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a25eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(args):\n",
    "    env = job_search_env.env()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e48f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"job_search_env\"\n",
    "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52ca8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"CandidateModelV0\", CandidateModelV0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b40f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use policy_map to map different policies to candidate and employer agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b2d7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need a custom preprocessor for the custom model with action masking\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=env_name, clip_actions=False)\n",
    "    .debugging(log_level=\"ERROR\")\n",
    "    .framework(framework=\"torch\")\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "\n",
    "    .training(model={\n",
    "                        \"custom_model\": CandidateModelV0,\n",
    "                        \"custom_model_config\": {},\n",
    "    })\n",
    ").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40f8ac80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candidate_0',\n",
       " 'candidate_1',\n",
       " 'candidate_2',\n",
       " 'candidate_3',\n",
       " 'candidate_4',\n",
       " 'employer_0',\n",
       " 'employer_1',\n",
       " 'employer_2',\n",
       " 'employer_3',\n",
       " 'employer_4']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_search_env.env().agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93b778a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"observation_space\"] = job_search_env.env().observation_space(\"candidate_0\")\n",
    "config[\"action_space\"] = job_search_env.env().action_space(\"candidate_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1efed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"observation_space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82e32e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"action_space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4594e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " '_fake_gpus': False,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'placement_strategy': 'PACK',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'env': 'job_search_env',\n",
       " 'env_config': {},\n",
       " 'observation_space': Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101)))),\n",
       " 'action_space': Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11)),\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'disable_env_checking': False,\n",
       " 'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'sample_async': False,\n",
       " 'enable_connectors': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'validate_workers_after_construction': True,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'restart_failed_sub_environments': False,\n",
       " 'num_consecutive_worker_failures_tolerance': 100,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'compress_observations': False,\n",
       " 'enable_tf1_exec_eagerly': False,\n",
       " 'sampler_perf_stats_ema_coef': None,\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'custom_model': __main__.CandidateModelV0,\n",
       "  'custom_model_config': {}},\n",
       " 'optimizer': {},\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_sample_timeout_s': 180.0,\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'evaluation_config': {},\n",
       " 'off_policy_estimation_methods': {},\n",
       " 'ope_split_batch_by_episode': True,\n",
       " 'evaluation_num_workers': 0,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'enable_async_evaluation': False,\n",
       " 'in_evaluation': False,\n",
       " 'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'metrics_episode_collection_timeout_s': 60.0,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_iteration': None,\n",
       " 'min_train_timesteps_per_iteration': 0,\n",
       " 'min_sample_timesteps_per_iteration': 0,\n",
       " 'export_native_model_files': False,\n",
       " 'logger_creator': None,\n",
       " 'logger_config': None,\n",
       " 'log_level': 'ERROR',\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'seed': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': -1,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'buffer_size': -1,\n",
       " 'prioritized_replay': -1,\n",
       " 'learning_starts': -1,\n",
       " 'replay_batch_size': -1,\n",
       " 'replay_sequence_length': None,\n",
       " 'replay_mode': -1,\n",
       " 'prioritized_replay_alpha': -1,\n",
       " 'prioritized_replay_beta': -1,\n",
       " 'prioritized_replay_eps': -1,\n",
       " 'min_time_s_per_reporting': -1,\n",
       " 'min_train_timesteps_per_reporting': -1,\n",
       " 'min_sample_timesteps_per_reporting': -1,\n",
       " 'input_evaluation': -1,\n",
       " 'lr_schedule': None,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'num_sgd_iter': 30,\n",
       " 'shuffle_sequences': True,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'vf_share_layers': -1,\n",
       " 'lambda': 1.0,\n",
       " 'input': 'sampler',\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       " 'create_env_on_driver': False,\n",
       " 'custom_eval_function': None,\n",
       " 'framework': 'torch',\n",
       " 'num_cpus_for_driver': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8afa733c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-12-14 02:36:30</td></tr>\n",
       "<tr><td>Running for: </td><td>00:05:37.18        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.7/62.4 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/31.84 GiB heap, 0.0/15.92 GiB objects (0.0/1.0 accelerator_type:P620)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_worker\n",
       "s</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_job_search_env_3dd05_00000</td><td>TERMINATED</td><td>128.36.108.32:740751</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         230.074</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 147.567</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">             392.288</td><td style=\"text-align: right;\">             0      </td></tr>\n",
       "<tr><td>PPO_job_search_env_3dd05_00001</td><td>TERMINATED</td><td>128.36.108.32:741214</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         227.652</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 144.082</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">             401.581</td><td style=\"text-align: right;\">             0      </td></tr>\n",
       "<tr><td>PPO_job_search_env_3dd05_00002</td><td>TERMINATED</td><td>128.36.108.32:741742</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         230.771</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 140.776</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">             418.087</td><td style=\"text-align: right;\">             4.81869</td></tr>\n",
       "<tr><td>PPO_job_search_env_3dd05_00003</td><td>TERMINATED</td><td>128.36.108.32:742241</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         230.846</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 134.444</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">             456.528</td><td style=\"text-align: right;\">             0      </td></tr>\n",
       "<tr><td>PPO_job_search_env_3dd05_00004</td><td>TERMINATED</td><td>128.36.108.32:742808</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         228.845</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 115.104</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">             372.375</td><td style=\"text-align: right;\">             0      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=740751)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m 2022-12-14 02:31:05,197\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m 2022-12-14 02:31:05,197\tINFO algorithm.py:457 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=740862)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=740861)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740862)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m 2022-12-14 02:31:17,622\tINFO trainable.py:164 -- Trainable.setup took 12.431 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=740751)\u001b[0m 2022-12-14 02:31:17,623\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m /usr/lib64/python3.10/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=740861)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(pid=741214)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m 2022-12-14 02:31:27,756\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m 2022-12-14 02:31:27,757\tINFO algorithm.py:457 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=741270)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=741271)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741271)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m 2022-12-14 02:31:39,451\tINFO trainable.py:164 -- Trainable.setup took 11.700 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=741214)\u001b[0m 2022-12-14 02:31:39,452\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m /usr/lib64/python3.10/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741270)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(pid=741742)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m 2022-12-14 02:31:49,332\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m 2022-12-14 02:31:49,333\tINFO algorithm.py:457 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=741799)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=741798)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741799)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=741742)\u001b[0m 2022-12-14 02:31:59,299\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m /usr/lib64/python3.10/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=741798)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(pid=742241)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m 2022-12-14 02:32:10,876\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m 2022-12-14 02:32:10,878\tINFO algorithm.py:457 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=742600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=742599)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742600)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m 2022-12-14 02:32:21,373\tINFO trainable.py:164 -- Trainable.setup took 10.502 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=742241)\u001b[0m 2022-12-14 02:32:21,375\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m /usr/lib64/python3.10/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742599)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(pid=742808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m 2022-12-14 02:32:31,079\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m 2022-12-14 02:32:31,080\tINFO algorithm.py:457 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=742962)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=742963)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742963)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m Orig space\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m Dict(action_mask:Box(0.0, 1.0, (122,), float32), observation:Dict(candidate_obs:Dict(accepted_offer:Dict(employer_0:Discrete(101), employer_1:Discrete(101), employer_2:Discrete(101), employer_3:Discrete(101), employer_4:Discrete(101)), counter_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), current_offers:Dict(employer_0:Tuple(Discrete(101), Discrete(11)), employer_1:Tuple(Discrete(101), Discrete(11)), employer_2:Tuple(Discrete(101), Discrete(11)), employer_3:Tuple(Discrete(101), Discrete(11)), employer_4:Tuple(Discrete(101), Discrete(11))), job_openings:Dict(employer_0:Discrete(2), employer_1:Discrete(2), employer_2:Discrete(2), employer_3:Discrete(2), employer_4:Discrete(2)), rejected_offers:Dict(employer_0:Tuple(Discrete(2), Discrete(101)), employer_1:Tuple(Discrete(2), Discrete(101)), employer_2:Tuple(Discrete(2), Discrete(101)), employer_3:Tuple(Discrete(2), Discrete(101)), employer_4:Tuple(Discrete(2), Discrete(101)))), employer_obs:Dict(accepted_offers:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), candidate_strengths:Dict(candidate_0:Discrete(101), candidate_1:Discrete(101), candidate_2:Discrete(101), candidate_3:Discrete(101), candidate_4:Discrete(101)), counter_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), declined_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), job_applicants:Dict(candidate_0:Discrete(2), candidate_1:Discrete(2), candidate_2:Discrete(2), candidate_3:Discrete(2), candidate_4:Discrete(2)), outstanding_offers:Dict(candidate_0:Tuple(Discrete(101), Discrete(11)), candidate_1:Tuple(Discrete(101), Discrete(11)), candidate_2:Tuple(Discrete(101), Discrete(11)), candidate_3:Tuple(Discrete(101), Discrete(11)), candidate_4:Tuple(Discrete(101), Discrete(11))), rejected_offers:Dict(candidate_0:Tuple(Discrete(2), Discrete(101)), candidate_1:Tuple(Discrete(2), Discrete(101)), candidate_2:Tuple(Discrete(2), Discrete(101)), candidate_3:Tuple(Discrete(2), Discrete(101)), candidate_4:Tuple(Discrete(2), Discrete(101))), remaining_budget:Discrete(101))))\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m Obs space\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m Box(-1.0, 1.0, (5048,), float32)\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m Flattened obs space\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m Box(0, 1, (4926,), int64)\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m Act space\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m Tuple(Discrete(5), Discrete(5), Discrete(101), Discrete(11))\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m Num outputs\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m Model config\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': <class '__main__.CandidateModelV0'>, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname             </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip      </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_recreated_workers</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                           </th><th style=\"text-align: right;\">   pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                  </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                                  </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_job_search_env_3dd05_00000</td><td style=\"text-align: right;\">                  12000</td><td>{&#x27;num_env_steps_sampled&#x27;: 12000, &#x27;num_env_steps_trained&#x27;: 12000, &#x27;num_agent_steps_sampled&#x27;: 12000, &#x27;num_agent_steps_trained&#x27;: 12000}</td><td>{}              </td><td>2022-12-14_02-35-27</td><td>True  </td><td style=\"text-align: right;\">                11</td><td>{}             </td><td style=\"text-align: right;\">             392.288</td><td style=\"text-align: right;\">              147.567</td><td style=\"text-align: right;\">             0      </td><td style=\"text-align: right;\">                  36</td><td style=\"text-align: right;\">             108</td><td>a8d107f66dd746fe8c2074b3c22fb651</td><td>rhino.zoo.cs.yale.edu</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 20.72351835953292, &#x27;cur_kl_coeff&#x27;: 0.20000000000000004, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 3.2625474049199013, &#x27;policy_loss&#x27;: -0.005354293065285811, &#x27;vf_loss&#x27;: 3.2659780230573427, &#x27;vf_explained_var&#x27;: 0.002066553215826711, &#x27;kl&#x27;: 0.00961846656098871, &#x27;entropy&#x27;: 3.246899869877805, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0}}, &#x27;num_env_steps_sampled&#x27;: 12000, &#x27;num_env_steps_trained&#x27;: 12000, &#x27;num_agent_steps_sampled&#x27;: 12000, &#x27;num_agent_steps_trained&#x27;: 12000}  </td><td style=\"text-align: right;\">                         3</td><td>128.36.108.32</td><td style=\"text-align: right;\">                    12000</td><td style=\"text-align: right;\">                    12000</td><td style=\"text-align: right;\">                  12000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                  12000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 38.937190082644626, &#x27;ram_util_percent&#x27;: 30.24297520661158}</td><td style=\"text-align: right;\">740751</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 13.186824583953562, &#x27;mean_inference_ms&#x27;: 8.316330841604797, &#x27;mean_action_processing_ms&#x27;: 0.8235492449690409, &#x27;mean_env_wait_ms&#x27;: 1.3278626748039262, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 392.2881058040335, &#x27;episode_reward_min&#x27;: 0.0, &#x27;episode_reward_mean&#x27;: 147.5669265827277, &#x27;episode_len_mean&#x27;: 11.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 36, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [40.74336065635986, 179.51987903583685, 51.56871329742377, 174.83464687854905, 229.74428961358865, 161.60485929838615, 132.41396331775337, 282.80631022384955, 30.36325550895048, 70.8263432781051, 25.64778013458824, 54.36684475844538, 164.44785682247962, 114.71557632879303, 54.01178108988922, 205.8785364144951, 80.17543871852229, 50.18485096230479, 218.23358877280197, 39.967364327793966, 0.0, 55.7983367953493, 87.29493369304865, 106.68934682721779, 172.6063692997957, 104.24829495321387, 0.0, 58.41187571022361, 136.9643714811686, 130.34152465699935, 145.1543255151412, 152.98513500453166, 7.365145964803514, 222.7481540171471, 119.34134003243318, 103.97203983088882, 292.8028139279175, 141.47182388392002, 134.2104498542677, 134.74934661089011, 251.43492986269084, 148.51475757395826, 186.7909282609303, 392.2881058040335, 103.57469705316387, 151.63902813227378, 199.70836892382152, 119.33637099937421, 94.44288613682316, 46.2653545914709, 113.40351533155919, 284.2442462482939, 88.55157057357515, 60.91554258258182, 131.60867435858427, 157.71445814499612, 184.89688315873278, 117.53443275057734, 62.612570685204005, 103.6073421940202, 188.47786348793662, 112.71023904648781, 60.78685331898305, 246.51671348097844, 143.4927712642553, 68.36754395851767, 261.9555940950879, 223.46832192288335, 123.346871060821, 236.53877147175737, 135.79147945044198, 207.02008975920333, 87.61709484078241, 228.95545382513217, 115.77933659727388, 163.31935830238825, 139.17578334724885, 165.9024599789715, 297.00569260862386, 210.871762021731, 190.12715811009565, 313.23980826459814, 116.32072761044729, 313.5010841955053, 161.4389740645756, 202.96825117784135, 189.68423642147843, 100.0301739191399, 155.3648485811858, 206.01761057850337, 219.36919535373352, 324.24481127102837, 196.88209209477162, 152.56845134361805, 25.361279481121624, 108.52103911086952, 123.92660267333297, 192.7278248961892, 239.88947912991404, 139.11342529077592], &#x27;episode_lengths&#x27;: [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 13.186824583953562, &#x27;mean_inference_ms&#x27;: 8.316330841604797, &#x27;mean_action_processing_ms&#x27;: 0.8235492449690409, &#x27;mean_env_wait_ms&#x27;: 1.3278626748039262, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}                                    </td><td style=\"text-align: right;\">             230.074</td><td style=\"text-align: right;\">           85.4745</td><td style=\"text-align: right;\">       230.074</td><td>{&#x27;training_iteration_time_ms&#x27;: 76683.371, &#x27;load_time_ms&#x27;: 0.729, &#x27;load_throughput&#x27;: 5488130.847, &#x27;learn_time_ms&#x27;: 71722.413, &#x27;learn_throughput&#x27;: 55.771, &#x27;synch_weights_time_ms&#x27;: 3.243}</td><td style=\"text-align: right;\"> 1671003327</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            12000</td><td style=\"text-align: right;\">                   3</td><td>3dd05_00000</td><td style=\"text-align: right;\">     12.4443 </td></tr>\n",
       "<tr><td>PPO_job_search_env_3dd05_00001</td><td style=\"text-align: right;\">                  12000</td><td>{&#x27;num_env_steps_sampled&#x27;: 12000, &#x27;num_env_steps_trained&#x27;: 12000, &#x27;num_agent_steps_sampled&#x27;: 12000, &#x27;num_agent_steps_trained&#x27;: 12000}</td><td>{}              </td><td>2022-12-14_02-35-27</td><td>True  </td><td style=\"text-align: right;\">                11</td><td>{}             </td><td style=\"text-align: right;\">             401.581</td><td style=\"text-align: right;\">              144.082</td><td style=\"text-align: right;\">             0      </td><td style=\"text-align: right;\">                  36</td><td style=\"text-align: right;\">             108</td><td>a247e2febb7e4b6f9b6c208d17feb294</td><td>rhino.zoo.cs.yale.edu</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 21.110224415025403, &#x27;cur_kl_coeff&#x27;: 0.20000000000000004, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 3.163854668665958, &#x27;policy_loss&#x27;: -0.014543076667694315, &#x27;vf_loss&#x27;: 3.175922465516675, &#x27;vf_explained_var&#x27;: 0.0012285226134843724, &#x27;kl&#x27;: 0.012376375345269558, &#x27;entropy&#x27;: 3.000019581343538, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0}}, &#x27;num_env_steps_sampled&#x27;: 12000, &#x27;num_env_steps_trained&#x27;: 12000, &#x27;num_agent_steps_sampled&#x27;: 12000, &#x27;num_agent_steps_trained&#x27;: 12000} </td><td style=\"text-align: right;\">                         3</td><td>128.36.108.32</td><td style=\"text-align: right;\">                    12000</td><td style=\"text-align: right;\">                    12000</td><td style=\"text-align: right;\">                  12000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                  12000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 38.86916666666666, &#x27;ram_util_percent&#x27;: 30.244166666666672}</td><td style=\"text-align: right;\">741214</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 13.348229923123052, &#x27;mean_inference_ms&#x27;: 8.705780999816042, &#x27;mean_action_processing_ms&#x27;: 0.8331518632824049, &#x27;mean_env_wait_ms&#x27;: 1.3255065748547232, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 401.5809049161718, &#x27;episode_reward_min&#x27;: 0.0, &#x27;episode_reward_mean&#x27;: 144.0820113742367, &#x27;episode_len_mean&#x27;: 11.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 36, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [88.43752314107331, 91.71342565329216, 98.25169389048929, 238.71595848016952, 148.51875081463467, 265.49752070008054, 54.42176870748298, 199.7919996443179, 88.18600885859836, 30.43999156729963, 0.0, 63.26006578698048, 274.8873747900881, 99.80259071246348, 53.649188574516714, 57.13652281125499, 107.42345948041977, 77.56909048037743, 127.15657169376003, 40.74336065635986, 20.596907086387, 145.48775034375166, 82.69495130833769, 44.765275090382126, 0.0, 162.91684596259552, 127.51657801274081, 61.72452640128145, 66.14666480186105, 19.188395913513276, 130.503960317514, 143.70880548509356, 18.55325738765417, 117.43050369195483, 149.0163649997507, 228.5567991962478, 182.430778961601, 126.55720548023217, 164.69816629928323, 175.1678816784415, 65.65165748839217, 130.23066164793966, 197.46871593392183, 114.55454593334174, 82.47431454683982, 230.19872346200196, 186.04223876318628, 169.12363124818006, 168.65567827951077, 202.24494913415367, 115.15140843691782, 258.998632431365, 281.4464675338661, 54.791757878512755, 316.4821923099448, 70.28240711861709, 173.0629247123884, 287.5586594919046, 32.48828937737697, 46.30105793496467, 233.56504857003975, 214.007421663089, 126.24855783755841, 89.64295328642491, 112.51405443316374, 158.7119577649122, 320.4906368107287, 102.21223477319317, 103.04298371470597, 103.32378022243412, 81.26302590356923, 401.5809049161718, 154.48906268286368, 105.75817619186984, 198.64372350655464, 133.74643925310608, 109.99670832650531, 316.8807637126238, 272.14236931666545, 59.54798865160287, 199.54301469291283, 102.57753770761836, 159.01458617518662, 320.75485351880707, 246.45787727217805, 133.1525052596353, 179.02936974038727, 154.12317573247256, 195.0949719756918, 146.45170834734975, 47.507803226870884, 98.56499147377718, 95.07812304372, 148.5588914236519, 97.59070500127544, 81.64156100752182, 346.3640218673735, 262.20143109079305, 131.95119317568296, 308.2606256273754], &#x27;episode_lengths&#x27;: [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 13.348229923123052, &#x27;mean_inference_ms&#x27;: 8.705780999816042, &#x27;mean_action_processing_ms&#x27;: 0.8331518632824049, &#x27;mean_env_wait_ms&#x27;: 1.3255065748547232, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}                                                 </td><td style=\"text-align: right;\">             227.652</td><td style=\"text-align: right;\">           83.8873</td><td style=\"text-align: right;\">       227.652</td><td>{&#x27;training_iteration_time_ms&#x27;: 75875.547, &#x27;load_time_ms&#x27;: 0.815, &#x27;load_throughput&#x27;: 4906096.891, &#x27;learn_time_ms&#x27;: 70834.063, &#x27;learn_throughput&#x27;: 56.47, &#x27;synch_weights_time_ms&#x27;: 3.262} </td><td style=\"text-align: right;\"> 1671003327</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            12000</td><td style=\"text-align: right;\">                   3</td><td>3dd05_00001</td><td style=\"text-align: right;\">     11.714  </td></tr>\n",
       "<tr><td>PPO_job_search_env_3dd05_00002</td><td style=\"text-align: right;\">                  12000</td><td>{&#x27;num_env_steps_sampled&#x27;: 12000, &#x27;num_env_steps_trained&#x27;: 12000, &#x27;num_agent_steps_sampled&#x27;: 12000, &#x27;num_agent_steps_trained&#x27;: 12000}</td><td>{}              </td><td>2022-12-14_02-35-50</td><td>True  </td><td style=\"text-align: right;\">                11</td><td>{}             </td><td style=\"text-align: right;\">             418.087</td><td style=\"text-align: right;\">              140.776</td><td style=\"text-align: right;\">             4.81869</td><td style=\"text-align: right;\">                  36</td><td style=\"text-align: right;\">             108</td><td>afad43f7a07b47d99116f33a09080985</td><td>rhino.zoo.cs.yale.edu</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 23.290901961506055, &#x27;cur_kl_coeff&#x27;: 0.20000000000000004, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 3.123967614225162, &#x27;policy_loss&#x27;: -0.011886170427365008, &#x27;vf_loss&#x27;: 3.1335880808932806, &#x27;vf_explained_var&#x27;: 0.002194843433236563, &#x27;kl&#x27;: 0.011328566949633405, &#x27;entropy&#x27;: 3.0606637959839196, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0}}, &#x27;num_env_steps_sampled&#x27;: 12000, &#x27;num_env_steps_trained&#x27;: 12000, &#x27;num_agent_steps_sampled&#x27;: 12000, &#x27;num_agent_steps_trained&#x27;: 12000}</td><td style=\"text-align: right;\">                         3</td><td>128.36.108.32</td><td style=\"text-align: right;\">                    12000</td><td style=\"text-align: right;\">                    12000</td><td style=\"text-align: right;\">                  12000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                  12000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 34.667226890756304, &#x27;ram_util_percent&#x27;: 28.91008403361344}</td><td style=\"text-align: right;\">741742</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 13.506641124431535, &#x27;mean_inference_ms&#x27;: 8.96506486959334, &#x27;mean_action_processing_ms&#x27;: 0.8369509511182307, &#x27;mean_env_wait_ms&#x27;: 1.3487762020223866, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 418.08656645127553, &#x27;episode_reward_min&#x27;: 4.818685923781022, &#x27;episode_reward_mean&#x27;: 140.77584566290034, &#x27;episode_len_mean&#x27;: 11.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 36, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [118.54487777365793, 62.94606066866789, 4.974769310910849, 119.42186317775747, 61.69645840428483, 137.50541872999455, 76.1408225665275, 115.47327922286743, 65.71326223323075, 130.77023515711292, 97.0184764576501, 190.02402945564978, 59.98167347282275, 69.822733239048, 33.84196810143435, 181.0941525649776, 214.40512623859945, 59.410032080149676, 234.08936232524155, 57.22308841696207, 28.92782736702963, 61.898567151008244, 171.50016761261855, 127.02197774486231, 128.47567977976826, 123.726235378844, 102.52396536678647, 8.63837598531476, 156.07726776434004, 87.2175201910166, 142.39429760344527, 124.86977545832825, 85.48481142422315, 199.83452884563098, 111.18468099524898, 183.72015603737913, 271.2720351984201, 100.26681388108918, 151.76317652555704, 233.2777751091402, 224.66023414299943, 262.7208553676762, 202.5797912452754, 100.48536531108826, 39.947951915893476, 142.1745396804247, 114.6559956932178, 120.43916501715168, 4.818685923781022, 111.34351889699794, 172.5691312013937, 91.74020378202003, 235.3617205784771, 77.53710982052156, 204.45830929231366, 156.39337955763705, 285.7233253194361, 136.2318016102463, 79.27239708089397, 63.54328740354801, 154.32232516311547, 63.01202712498962, 168.27435661370794, 27.149848376054926, 190.09551063247343, 9.143114133686607, 195.54675276764408, 139.99203046662535, 82.34074505777025, 38.19319463428866, 172.95523217954917, 139.12901654831188, 209.62597111775773, 153.96406700030187, 308.47298308785304, 201.14384679271592, 100.3338860285381, 167.3580148985138, 40.671557362225734, 298.30024820576625, 293.8190194646929, 163.75079973781607, 67.97547652922685, 82.1582600858464, 64.47657535959188, 261.1149327155058, 149.8221041913891, 331.6792155562905, 236.47873733265837, 146.4972731732015, 188.71295970182004, 115.82670358648876, 246.82158798257987, 225.06087032012405, 179.63150274687132, 187.24656463447116, 18.96329204395288, 166.66597199543273, 155.9413306303183, 418.08656645127553], &#x27;episode_lengths&#x27;: [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 13.506641124431535, &#x27;mean_inference_ms&#x27;: 8.96506486959334, &#x27;mean_action_processing_ms&#x27;: 0.8369509511182307, &#x27;mean_env_wait_ms&#x27;: 1.3487762020223866, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}</td><td style=\"text-align: right;\">             230.771</td><td style=\"text-align: right;\">           83.3473</td><td style=\"text-align: right;\">       230.771</td><td>{&#x27;training_iteration_time_ms&#x27;: 76915.032, &#x27;load_time_ms&#x27;: 0.914, &#x27;load_throughput&#x27;: 4378568.769, &#x27;learn_time_ms&#x27;: 71843.61, &#x27;learn_throughput&#x27;: 55.676, &#x27;synch_weights_time_ms&#x27;: 3.111} </td><td style=\"text-align: right;\"> 1671003350</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            12000</td><td style=\"text-align: right;\">                   3</td><td>3dd05_00002</td><td style=\"text-align: right;\">      9.98543</td></tr>\n",
       "<tr><td>PPO_job_search_env_3dd05_00003</td><td style=\"text-align: right;\">                  12000</td><td>{&#x27;num_env_steps_sampled&#x27;: 12000, &#x27;num_env_steps_trained&#x27;: 12000, &#x27;num_agent_steps_sampled&#x27;: 12000, &#x27;num_agent_steps_trained&#x27;: 12000}</td><td>{}              </td><td>2022-12-14_02-36-12</td><td>True  </td><td style=\"text-align: right;\">                11</td><td>{}             </td><td style=\"text-align: right;\">             456.528</td><td style=\"text-align: right;\">              134.444</td><td style=\"text-align: right;\">             0      </td><td style=\"text-align: right;\">                  36</td><td style=\"text-align: right;\">             108</td><td>ecfa2431807b4d0c8ca8d36f39da3dfc</td><td>rhino.zoo.cs.yale.edu</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 19.00317257142836, &#x27;cur_kl_coeff&#x27;: 0.20000000000000004, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 3.542846171702108, &#x27;policy_loss&#x27;: -0.013443592607334095, &#x27;vf_loss&#x27;: 3.554204406276826, &#x27;vf_explained_var&#x27;: 0.001945622762044271, &#x27;kl&#x27;: 0.010426837421254331, &#x27;entropy&#x27;: 3.267688058525003, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0}}, &#x27;num_env_steps_sampled&#x27;: 12000, &#x27;num_env_steps_trained&#x27;: 12000, &#x27;num_agent_steps_sampled&#x27;: 12000, &#x27;num_agent_steps_trained&#x27;: 12000}   </td><td style=\"text-align: right;\">                         3</td><td>128.36.108.32</td><td style=\"text-align: right;\">                    12000</td><td style=\"text-align: right;\">                    12000</td><td style=\"text-align: right;\">                  12000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                  12000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 28.67043478260869, &#x27;ram_util_percent&#x27;: 26.784347826086957}</td><td style=\"text-align: right;\">742241</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 13.563815505022362, &#x27;mean_inference_ms&#x27;: 8.856844790321963, &#x27;mean_action_processing_ms&#x27;: 0.8343329047284894, &#x27;mean_env_wait_ms&#x27;: 1.3368412076785672, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 456.52832643435886, &#x27;episode_reward_min&#x27;: 0.0, &#x27;episode_reward_mean&#x27;: 134.44427482737018, &#x27;episode_len_mean&#x27;: 11.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 36, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [113.89218826860548, 72.0844073150982, 117.80248972327657, 57.1974101521975, 27.073574481147478, 120.15026875906099, 41.321607084653024, 115.97042619476954, 124.2543360296527, 89.88837001172108, 0.0, 60.07141185896328, 53.43359106487621, 0.0, 143.73735412221046, 92.6013506654284, 0.0, 91.26172804166973, 117.8258882403589, 122.51445119246904, 186.21722229669075, 131.28764445345496, 59.60479429867184, 115.98676337465446, 29.931972789115644, 52.73570203897649, 84.85890422418714, 38.89086743771025, 142.728903348372, 197.9073994526809, 72.97941763438709, 121.36946786645134, 187.90205253479184, 107.39784277244934, 160.0776890128612, 141.82524222394918, 148.0159781971226, 100.87379183334384, 185.86233036384442, 195.31440903215426, 52.51935029661597, 55.90847414431485, 8.227024747918819, 119.73449466516527, 134.3753128627984, 150.27508285435883, 148.26251676207846, 114.12610932223285, 138.72283415489707, 50.50067689936541, 174.90246878065247, 184.5790857155179, 171.2455201279302, 93.36567091067843, 170.6677589026196, 120.49844739028684, 170.1413688606598, 52.67170260676262, 124.3789237829296, 203.68763052831042, 44.79721133265485, 89.02803728280325, 25.65799456718657, 345.9541939160989, 407.517112858243, 162.71785116135337, 76.91800337548783, 202.92205095896648, 405.3052323116242, 79.04428725139503, 64.36652228652744, 78.8922993755035, 241.9141719709802, 206.15852537257604, 255.8511592856387, 202.99336886306634, 239.8857083331031, 35.534066506506065, 137.77065214093923, 245.84610200888113, 216.95666216859922, 169.263025856825, 47.011569988107524, 160.8803334164692, 107.60546158486927, 180.683086308512, 164.3005761241095, 124.10739686124955, 456.52832643435886, 124.7716016870801, 166.86995458910033, 288.94025741662114, 90.64740365809698, 172.5204084577565, 152.2155039905947, 132.39276052904867, 114.90342203163075, 60.964187150313485, 189.92227994936624, 312.22901066665156], &#x27;episode_lengths&#x27;: [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 13.563815505022362, &#x27;mean_inference_ms&#x27;: 8.856844790321963, &#x27;mean_action_processing_ms&#x27;: 0.8343329047284894, &#x27;mean_env_wait_ms&#x27;: 1.3368412076785672, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}                                                              </td><td style=\"text-align: right;\">             230.846</td><td style=\"text-align: right;\">           80.6556</td><td style=\"text-align: right;\">       230.846</td><td>{&#x27;training_iteration_time_ms&#x27;: 76940.489, &#x27;load_time_ms&#x27;: 0.99, &#x27;load_throughput&#x27;: 4038809.822, &#x27;learn_time_ms&#x27;: 71876.934, &#x27;learn_throughput&#x27;: 55.651, &#x27;synch_weights_time_ms&#x27;: 3.05}  </td><td style=\"text-align: right;\"> 1671003372</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            12000</td><td style=\"text-align: right;\">                   3</td><td>3dd05_00003</td><td style=\"text-align: right;\">     10.5179 </td></tr>\n",
       "<tr><td>PPO_job_search_env_3dd05_00004</td><td style=\"text-align: right;\">                  12000</td><td>{&#x27;num_env_steps_sampled&#x27;: 12000, &#x27;num_env_steps_trained&#x27;: 12000, &#x27;num_agent_steps_sampled&#x27;: 12000, &#x27;num_agent_steps_trained&#x27;: 12000}</td><td>{}              </td><td>2022-12-14_02-36-30</td><td>True  </td><td style=\"text-align: right;\">                11</td><td>{}             </td><td style=\"text-align: right;\">             372.375</td><td style=\"text-align: right;\">              115.104</td><td style=\"text-align: right;\">             0      </td><td style=\"text-align: right;\">                  36</td><td style=\"text-align: right;\">             108</td><td>b75136b17b0e4d2192f5c5e5011b5628</td><td>rhino.zoo.cs.yale.edu</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 22.13207455578671, &#x27;cur_kl_coeff&#x27;: 0.20000000000000004, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 3.265190464065921, &#x27;policy_loss&#x27;: -0.014623811440442199, &#x27;vf_loss&#x27;: 3.2772073973891556, &#x27;vf_explained_var&#x27;: 0.004498706902227094, &#x27;kl&#x27;: 0.013034450966406732, &#x27;entropy&#x27;: 3.2277742452518914, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0}}, &#x27;num_env_steps_sampled&#x27;: 12000, &#x27;num_env_steps_trained&#x27;: 12000, &#x27;num_agent_steps_sampled&#x27;: 12000, &#x27;num_agent_steps_trained&#x27;: 12000} </td><td style=\"text-align: right;\">                         3</td><td>128.36.108.32</td><td style=\"text-align: right;\">                    12000</td><td style=\"text-align: right;\">                    12000</td><td style=\"text-align: right;\">                  12000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                  12000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 22.202727272727273, &#x27;ram_util_percent&#x27;: 24.25}            </td><td style=\"text-align: right;\">742808</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 13.485370154113657, &#x27;mean_inference_ms&#x27;: 9.206481629149907, &#x27;mean_action_processing_ms&#x27;: 0.8227073411276342, &#x27;mean_env_wait_ms&#x27;: 1.324264444212131, &#x27;mean_env_render_ms&#x27;: 0.0} </td><td>{&#x27;episode_reward_max&#x27;: 372.37533705542677, &#x27;episode_reward_min&#x27;: 0.0, &#x27;episode_reward_mean&#x27;: 115.10395199652987, &#x27;episode_len_mean&#x27;: 11.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 36, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [132.35542902045975, 40.671353811612605, 141.07860363148706, 208.76801281201784, 52.60770975056689, 45.2179456787315, 142.18609839909016, 8.528175961561455, 117.68455085629239, 106.68641525713863, 64.5504500545978, 69.84126984126983, 138.11870210491605, 0.0, 62.32123721737226, 56.89679194955245, 87.65141753173856, 69.59850526015008, 111.89565632798059, 109.80891861241241, 86.30463370429048, 252.53111749112148, 81.00505774146885, 97.11806833376548, 58.98655040080007, 14.73391808497822, 86.01354373949124, 188.79280499156397, 108.22939326001739, 34.16427255954325, 119.21351307714573, 28.24000966287492, 65.65165748839217, 112.95279063815008, 201.23855595773125, 83.3657150006819, 135.84230802133575, 50.51583568379142, 177.9203602159083, 0.0, 101.11723613065669, 29.094473563704103, 15.634989262862668, 11.50626915448768, 117.9370838525113, 196.6017639841508, 63.965290963069755, 13.502945272472305, 270.17947188868084, 112.03721959660857, 61.74896398911153, 115.92456186750005, 76.0020381474405, 39.71740332115076, 145.83338921153103, 175.4577148796446, 112.10323351629056, 248.4110901447745, 171.01563189617295, 33.58493835370395, 114.5485742113774, 146.55962729151685, 25.398824314973762, 25.072837326990683, 86.77674064291259, 68.63158127663498, 26.797313517054135, 321.2431317539243, 372.37533705542677, 121.46657950424523, 90.85279217402687, 228.15380381272047, 166.30882239659655, 102.23290270167843, 185.5103917192179, 193.08937780846003, 110.11602122008055, 95.13498635057691, 147.59607534474893, 129.2257002927653, 136.61853345753167, 128.03799465662254, 65.34608228346947, 289.7267411293733, 221.65344034479756, 219.92344921837932, 245.0685724234033, 100.2250841156964, 119.30023426429872, 204.62425262174253, 100.31924355017034, 324.45469617676446, 59.13932889765688, 163.29195724089402, 99.46156117109962, 39.13904187045409, 100.73907854594471, 77.22779717598036, 53.589903275221566, 144.75372738903147], &#x27;episode_lengths&#x27;: [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 13.485370154113657, &#x27;mean_inference_ms&#x27;: 9.206481629149907, &#x27;mean_action_processing_ms&#x27;: 0.8227073411276342, &#x27;mean_env_wait_ms&#x27;: 1.324264444212131, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}                                       </td><td style=\"text-align: right;\">             228.845</td><td style=\"text-align: right;\">           77.5126</td><td style=\"text-align: right;\">       228.845</td><td>{&#x27;training_iteration_time_ms&#x27;: 76273.454, &#x27;load_time_ms&#x27;: 0.82, &#x27;load_throughput&#x27;: 4875680.325, &#x27;learn_time_ms&#x27;: 71165.79, &#x27;learn_throughput&#x27;: 56.207, &#x27;synch_weights_time_ms&#x27;: 3.449}  </td><td style=\"text-align: right;\"> 1671003390</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            12000</td><td style=\"text-align: right;\">                   3</td><td>3dd05_00004</td><td style=\"text-align: right;\">     10.6603 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m 2022-12-14 02:32:41,719\tINFO trainable.py:164 -- Trainable.setup took 10.645 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=742808)\u001b[0m 2022-12-14 02:32:41,720\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m /usr/lib64/python3.10/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=742962)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "2022-12-14 02:36:31,111\tINFO tune.py:777 -- Total run time: 338.70 seconds (337.02 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fc60c4693f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\n",
    "    \"PPO\",\n",
    "    name=\"PPO\",\n",
    "    stop={\"timesteps_total\": 10000},\n",
    "    checkpoint_freq=10,\n",
    "    local_dir=\"~/ray_results/\" + env_name,\n",
    "    config=config,\n",
    "    num_samples=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8cfa6",
   "metadata": {},
   "source": [
    "### Use gym.space instead of gymnasium.spaces to fix this AHHHHH\n",
    "```\n",
    "from gym.spaces import Discrete, Dict, Tuple\n",
    "\n",
    "from gym.spaces.utils import flatten, flatdim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6119029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Dict, Tuple, Box\n",
    "\n",
    "from gym.spaces.utils import flatten, flatdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5ad4d823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(10)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = Dict({\"test\": Discrete(10)})\n",
    "space[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "03395401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(flatdim(Tuple((Discrete(4), Discrete(2)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3bc19a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66815686, 0.8099814 , 0.6728993 , 0.3833307 , 0.83226   ,\n",
       "       0.01675155, 0.36507058, 0.9905923 , 0.5650419 , 0.37177613],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Box(0.0, 1.0, shape=(10,)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5640cd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8192585 , 0.21756119, 0.7595661 , 0.06519676, 0.5593857 ,\n",
       "       0.6580883 , 0.89733326, 0.01662113, 0.39561164, 0.0882608 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Box(0.0, 1, shape=(10,)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "335253fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "09647c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten(Tuple((Discrete(10), Discrete(5))), (0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d024f0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_mask = torch.from_numpy(np.array([1, 0]))\n",
    "action_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "46f4964b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00, -3.4000e+38])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_mask = torch.clamp(torch.log(action_mask), min=FLOAT_MIN)\n",
    "inf_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e682924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000e+00, -3.4000e+38])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([1,1])\n",
    "logits + inf_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f502255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0, 1]) + torch.tensor([1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb93663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24c4d728",
   "metadata": {},
   "source": [
    "# Run 1\n",
    "Got the error: \n",
    "```\n",
    "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
    "```\n",
    "\n",
    "# Run 2\n",
    "\n",
    "```\n",
    "AssertionError: homogenization only supports Discrete and Box spaces\n",
    "```\n",
    "\n",
    "# Run 3\n",
    "\n",
    "Same error\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
    "    future_result = ray.get(ready_future)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
    "    return func(*args, **kwargs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2291, in get\n",
    "    raise value\n",
    "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::PPO.__init__() (pid=1879526, ip=128.36.108.57, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 139, in __init__\n",
    "    self.add_workers(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 490, in add_workers\n",
    "    self.foreach_worker(lambda w: w.assert_healthy())\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 620, in foreach_worker\n",
    "    remote_results = ray.get([w.apply.remote(func) for w in self.remote_workers()])\n",
    "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=1879573, ip=128.36.108.57, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa145f35d50>)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 492, in __init__\n",
    "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
    "  File \"/tmp/ipykernel_1861406/2067092441.py\", line 2, in <lambda>\n",
    "  File \"/tmp/ipykernel_1861406/841127696.py\", line 3, in env_creator\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/supersuit/multiagent_wrappers/padding_wrappers.py\", line 33, in pad_observations_v0\n",
    "    homogenize_ops.check_homogenize_spaces(spaces)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/supersuit/utils/action_transforms/homogenize_ops.py\", line 30, in check_homogenize_spaces\n",
    "    assert False, \"homogenization only supports Discrete and Box spaces\"\n",
    "AssertionError: homogenization only supports Discrete and Box spaces\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "ray::PPO.__init__() (pid=1879526, ip=128.36.108.57, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 414, in __init__\n",
    "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
    "    self.setup(copy.deepcopy(self.config))\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 549, in setup\n",
    "    raise e.args[0].args[2]\n",
    "AssertionError: homogenization only supports Discrete and Box spaces\n",
    "```\n",
    "\n",
    "i.e. I cannot use the SuperSuit wrapper to fix the issue of observation spaces for all agents needing to be identical.\n",
    "\n",
    "# Run 4\n",
    "\n",
    "Fixed the issue by making all observation spaces and action spaces the same for all agents.\n",
    "\n",
    "New issue:\n",
    "\n",
    "```\n",
    "Failure # 1 (occurred at 2022-12-12_17-37-17)\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
    "    future_result = ray.get(ready_future)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
    "    return func(*args, **kwargs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2291, in get\n",
    "    raise value\n",
    "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::PPO.__init__() (pid=499392, ip=128.36.232.21, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 139, in __init__\n",
    "    self.add_workers(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 490, in add_workers\n",
    "    self.foreach_worker(lambda w: w.assert_healthy())\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 620, in foreach_worker\n",
    "    remote_results = ray.get([w.apply.remote(func) for w in self.remote_workers()])\n",
    "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=499452, ip=128.36.232.21, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8059b28df0>)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 567, in __init__\n",
    "    self.policy_dict = _determine_spaces_for_multi_agent_dict(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2121, in _determine_spaces_for_multi_agent_dict\n",
    "    raise ValueError(\n",
    "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "ray::PPO.__init__() (pid=499392, ip=128.36.232.21, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 414, in __init__\n",
    "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
    "    self.setup(copy.deepcopy(self.config))\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 549, in setup\n",
    "    raise e.args[0].args[2]\n",
    "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
    "```\n",
    "\n",
    "# Run 5\n",
    "\n",
    "```\n",
    "Failure # 1 (occurred at 2022-12-12_20-29-44)\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
    "    future_result = ray.get(ready_future)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
    "    return func(*args, **kwargs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2291, in get\n",
    "    raise value\n",
    "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::PPO.__init__() (pid=2054378, ip=128.36.108.57, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 139, in __init__\n",
    "    self.add_workers(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 490, in add_workers\n",
    "    self.foreach_worker(lambda w: w.assert_healthy())\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 620, in foreach_worker\n",
    "    remote_results = ray.get([w.apply.remote(func) for w in self.remote_workers()])\n",
    "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::RolloutWorker.__init__() (pid=2054502, ip=128.36.108.57, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0abd339d50>)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 625, in __init__\n",
    "    self._build_policy_map(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1882, in _build_policy_map\n",
    "    preprocessor = ModelCatalog.get_preprocessor_for_space(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 815, in get_preprocessor_for_space\n",
    "    prep = cls(observation_space, options)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py\", line 42, in __init__\n",
    "    self._size = int(np.product(self.shape))\n",
    "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "ray::PPO.__init__() (pid=2054378, ip=128.36.108.57, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 414, in __init__\n",
    "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
    "    self.setup(copy.deepcopy(self.config))\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 549, in setup\n",
    "    raise e.args[0].args[2]\n",
    "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
    "```\n",
    "\n",
    "Note: the `Dict` space does not have a shape <https://github.com/openai/gym/blob/master/gym/spaces/dict.py#L118>\n",
    "\n",
    "Nope, the issue was I was using the wrong spaces from gymnasium.spaces, when instead RLlib assumes using gym.spaces spaces\n",
    "\n",
    "# Run 6\n",
    "\n",
    "```\n",
    "2022-12-12 21:59:59,682\tERROR trial_runner.py:993 -- Trial PPO_job_search_30f4a_00000: Error processing event.\n",
    "ray.exceptions.RayTaskError(ValueError): ray::PPO.train() (pid=1671497, ip=128.36.232.24, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 355, in train\n",
    "    raise skipped from exception_cause(skipped)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 352, in train\n",
    "    result = self.step()\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 772, in step\n",
    "    results, train_iter_ctx = self._run_one_training_iteration()\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2953, in _run_one_training_iteration\n",
    "    num_recreated += self.try_recover_from_step_attempt(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2617, in try_recover_from_step_attempt\n",
    "    raise error\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2948, in _run_one_training_iteration\n",
    "    results = self.training_step()\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo.py\", line 408, in training_step\n",
    "    train_batch = synchronous_parallel_sample(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 100, in synchronous_parallel_sample\n",
    "    sample_batches = ray.get(\n",
    "ray.exceptions.RayTaskError(ValueError): ray::RolloutWorker.sample() (pid=1671528, ip=128.36.232.24, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f25d5759db0>)\n",
    "ValueError: The two structures don't have the same nested structure.\n",
    "\n",
    "First structure: type=dict str={'observation': {'candidate_obs': {'job_openings': {'employer_0': 1}, 'accepted_offer': {'employer_0': 0}, 'current_offers': {'employer_0': (0, 0)}, 'rejected_offers': {'employer_0': (0, 0)}, 'counter_offers': {'employer_0': (0, 0)}}, 'employer_obs': {'candidate_strengths': {'candidate_0': 0}, 'job_applicants': {'candidate_0': 0}, 'outstanding_offers': {'candidate_0': (0, 0)}, 'accepted_offers': {'candidate_0': 0}, 'declined_offers': {'candidate_0': (0, 0)}, 'counter_offers': {'candidate_0': (0, 0)}, 'rejected_offers': {'candidate_0': (0, 0)}, 'remaining_budget': 100}}, 'action_mask': array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0])}\n",
    "\n",
    "Second structure: type=OrderedDict str=OrderedDict([('candidate_obs', OrderedDict([('accepted_offer', OrderedDict([('employer_0', 56)])), ('counter_offers', OrderedDict([('employer_0', (99, 8))])), ('current_offers', OrderedDict([('employer_0', (28, 10))])), ('job_openings', OrderedDict([('employer_0', 1)])), ('rejected_offers', OrderedDict([('employer_0', (1, 49))]))])), ('employer_obs', OrderedDict([('accepted_offers', OrderedDict([('candidate_0', 1)])), ('candidate_strengths', OrderedDict([('candidate_0', 79)])), ('counter_offers', OrderedDict([('candidate_0', (66, 5))])), ('declined_offers', OrderedDict([('candidate_0', (1, 7))])), ('job_applicants', OrderedDict([('candidate_0', 1)])), ('outstanding_offers', OrderedDict([('candidate_0', (28, 0))])), ('rejected_offers', OrderedDict([('candidate_0', (0, 4))])), ('remaining_budget', 96)]))])\n",
    "\n",
    "More specifically: Substructure \"type=OrderedDict str=OrderedDict([('accepted_offer', OrderedDict([('employer_0', 56)])), ('counter_offers', OrderedDict([('employer_0', (99, 8))])), ('current_offers', OrderedDict([('employer_0', (28, 10))])), ('job_openings', OrderedDict([('employer_0', 1)])), ('rejected_offers', OrderedDict([('employer_0', (1, 49))]))])\" is a sequence, while substructure \"type=ndarray str=[0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0]\" is not\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "ray::RolloutWorker.sample() (pid=1671528, ip=128.36.232.24, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f25d5759db0>)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 828, in sample\n",
    "    batches = [self.input_reader.next()]\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
    "    batches = [self.get_data()]\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 285, in get_data\n",
    "    item = next(self._env_runner)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 671, in _env_runner\n",
    "    active_envs, to_eval, outputs = _process_observations(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 922, in _process_observations\n",
    "    prep_obs = preprocessor.transform(raw_obs)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py\", line 283, in transform\n",
    "    self.check_shape(observation)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py\", line 69, in check_shape\n",
    "    observation = convert_element_to_space_type(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/utils/spaces/space_utils.py\", line 359, in convert_element_to_space_type\n",
    "    return tree.map_structure(map_, element, sampled_element, check_types=False)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/tree/__init__.py\", line 428, in map_structure\n",
    "    assert_same_structure(structures[0], other, check_types=check_types)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/tree/__init__.py\", line 284, in assert_same_structure\n",
    "    raise type(e)(\"%s\\n\"\n",
    "ValueError: The two structures don't have the same nested structure.\n",
    "\n",
    "First structure: type=dict str={'observation': {'candidate_obs': {'job_openings': {'employer_0': 1}, 'accepted_offer': {'employer_0': 0}, 'current_offers': {'employer_0': (0, 0)}, 'rejected_offers': {'employer_0': (0, 0)}, 'counter_offers': {'employer_0': (0, 0)}}, 'employer_obs': {'candidate_strengths': {'candidate_0': 0}, 'job_applicants': {'candidate_0': 0}, 'outstanding_offers': {'candidate_0': (0, 0)}, 'accepted_offers': {'candidate_0': 0}, 'declined_offers': {'candidate_0': (0, 0)}, 'counter_offers': {'candidate_0': (0, 0)}, 'rejected_offers': {'candidate_0': (0, 0)}, 'remaining_budget': 100}}, 'action_mask': array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0])}\n",
    "\n",
    "Second structure: type=OrderedDict str=OrderedDict([('candidate_obs', OrderedDict([('accepted_offer', OrderedDict([('employer_0', 56)])), ('counter_offers', OrderedDict([('employer_0', (99, 8))])), ('current_offers', OrderedDict([('employer_0', (28, 10))])), ('job_openings', OrderedDict([('employer_0', 1)])), ('rejected_offers', OrderedDict([('employer_0', (1, 49))]))])), ('employer_obs', OrderedDict([('accepted_offers', OrderedDict([('candidate_0', 1)])), ('candidate_strengths', OrderedDict([('candidate_0', 79)])), ('counter_offers', OrderedDict([('candidate_0', (66, 5))])), ('declined_offers', OrderedDict([('candidate_0', (1, 7))])), ('job_applicants', OrderedDict([('candidate_0', 1)])), ('outstanding_offers', OrderedDict([('candidate_0', (28, 0))])), ('rejected_offers', OrderedDict([('candidate_0', (0, 4))])), ('remaining_budget', 96)]))])\n",
    "\n",
    "More specifically: Substructure \"type=OrderedDict str=OrderedDict([('accepted_offer', OrderedDict([('employer_0', 56)])), ('counter_offers', OrderedDict([('employer_0', (99, 8))])), ('current_offers', OrderedDict([('employer_0', (28, 10))])), ('job_openings', OrderedDict([('employer_0', 1)])), ('rejected_offers', OrderedDict([('employer_0', (1, 49))]))])\" is a sequence, while substructure \"type=ndarray str=[0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0]\" is not\n",
    "Entire first structure:\n",
    "{'observation': {'candidate_obs': {'job_openings': {'employer_0': .}, 'accepted_offer': {'employer_0': .}, 'current_offers': {'employer_0': (., .)}, 'rejected_offers': {'employer_0': (., .)}, 'counter_offers': {'employer_0': (., .)}}, 'employer_obs': {'candidate_strengths': {'candidate_0': .}, 'job_applicants': {'candidate_0': .}, 'outstanding_offers': {'candidate_0': (., .)}, 'accepted_offers': {'candidate_0': .}, 'declined_offers': {'candidate_0': (., .)}, 'counter_offers': {'candidate_0': (., .)}, 'rejected_offers': {'candidate_0': (., .)}, 'remaining_budget': .}}, 'action_mask': .}\n",
    "Entire second structure:\n",
    "OrderedDict([('candidate_obs', OrderedDict([('accepted_offer', OrderedDict([('employer_0', .)])), ('counter_offers', OrderedDict([('employer_0', (., .))])), ('current_offers', OrderedDict([('employer_0', (., .))])), ('job_openings', OrderedDict([('employer_0', .)])), ('rejected_offers', OrderedDict([('employer_0', (., .))]))])), ('employer_obs', OrderedDict([('accepted_offers', OrderedDict([('candidate_0', .)])), ('candidate_strengths', OrderedDict([('candidate_0', .)])), ('counter_offers', OrderedDict([('candidate_0', (., .))])), ('declined_offers', OrderedDict([('candidate_0', (., .))])), ('job_applicants', OrderedDict([('candidate_0', .)])), ('outstanding_offers', OrderedDict([('candidate_0', (., .))])), ('rejected_offers', OrderedDict([('candidate_0', (., .))])), ('remaining_budget', .)]))])\n",
    "```\n",
    "\n",
    "# Run 7\n",
    "\n",
    "Issues with observation/action_mask dictionary structure. Fixed by updating the observation space definition to also include the action mask.\n",
    "\n",
    "# Run 8\n",
    "\n",
    "```\n",
    "Failure # 1 (occurred at 2022-12-13_13-53-45)\n",
    "ray::PPO.train() (pid=2035053, ip=128.36.232.24, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 355, in train\n",
    "    raise skipped from exception_cause(skipped)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 352, in train\n",
    "    result = self.step()\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 772, in step\n",
    "    results, train_iter_ctx = self._run_one_training_iteration()\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2953, in _run_one_training_iteration\n",
    "    num_recreated += self.try_recover_from_step_attempt(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2617, in try_recover_from_step_attempt\n",
    "    raise error\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2948, in _run_one_training_iteration\n",
    "    results = self.training_step()\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo.py\", line 408, in training_step\n",
    "    train_batch = synchronous_parallel_sample(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 100, in synchronous_parallel_sample\n",
    "    sample_batches = ray.get(\n",
    "ray.exceptions.RayTaskError(ValueError): ray::RolloutWorker.sample() (pid=2035175, ip=128.36.232.24, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa1c5255d80>)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 828, in sample\n",
    "    batches = [self.input_reader.next()]\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
    "    batches = [self.get_data()]\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 285, in get_data\n",
    "    item = next(self._env_runner)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 721, in _env_runner\n",
    "    base_env.send_actions(actions_to_send)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 615, in send_actions\n",
    "    raise e\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 608, in send_actions\n",
    "    obs, rewards, dones, infos = env.step(agent_dict)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py\", line 184, in step\n",
    "    obss, rews, dones, infos = self.par_env.step(action_dict)\n",
    "ValueError: too many values to unpack (expected 4)\n",
    "```\n",
    "\n",
    "# Run 9\n",
    "\n",
    "```\n",
    "Failure # 1 (occurred at 2022-12-13_14-18-31)\n",
    "ray::PPO.train() (pid=2045783, ip=128.36.232.24, repr=PPO)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 355, in train\n",
    "    raise skipped from exception_cause(skipped)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 352, in train\n",
    "    result = self.step()\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 772, in step\n",
    "    results, train_iter_ctx = self._run_one_training_iteration()\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2953, in _run_one_training_iteration\n",
    "    num_recreated += self.try_recover_from_step_attempt(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2617, in try_recover_from_step_attempt\n",
    "    raise error\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2948, in _run_one_training_iteration\n",
    "    results = self.training_step()\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo.py\", line 408, in training_step\n",
    "    train_batch = synchronous_parallel_sample(\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 100, in synchronous_parallel_sample\n",
    "    sample_batches = ray.get(\n",
    "ray.exceptions.RayTaskError(KeyError): ray::RolloutWorker.sample() (pid=2045906, ip=128.36.232.24, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f68b792dde0>)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 828, in sample\n",
    "    batches = [self.input_reader.next()]\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
    "    batches = [self.get_data()]\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 285, in get_data\n",
    "    item = next(self._env_runner)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 721, in _env_runner\n",
    "    base_env.send_actions(actions_to_send)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 615, in send_actions\n",
    "    raise e\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 608, in send_actions\n",
    "    obs, rewards, dones, infos = env.step(agent_dict)\n",
    "  File \"/home/accts/ahc49/.local/lib/python3.10/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py\", line 184, in step\n",
    "    obss, rews, dones, infos = self.par_env.step(action_dict)\n",
    "  File \"/home/accts/ahc49/csec491/salary-negotation/environment/job_search_environment.py\", line 394, in step\n",
    "    action, target_index, new_offer_value, new_deadline = actions[agent]\n",
    "KeyError: 'employer_0'\n",
    "```\n",
    "\n",
    "I didn't actually specify in the config to use the custom model sigh :(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c65841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
